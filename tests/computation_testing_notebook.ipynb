{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation Notebook \n",
    "\n",
    "This notebook is meant to show and compare the computational costs with the grid-based approach (searching all possible gridpoints) versus the gridless approach (binary search) to find decision boundary points. \n",
    "\n",
    "We show the times that are associated with each of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numba\n",
    "from numba import cuda\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from scipy.linalg import norm\n",
    "import numpy as np \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "import random \n",
    "from scipy.interpolate import RBFInterpolator\n",
    "from scipy.spatial import KDTree\n",
    "import numba \n",
    "\n",
    "random.seed(0)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_point(point, contour):\n",
    "    \"\"\"\n",
    "    Finds the closest point on a contour to a given reference point.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    point : array-like or tuple\n",
    "        A single point (e.g., [x, y]) for which the nearest contour point is to be found.\n",
    "    \n",
    "    contour : array-like of shape (n_points, n_dimensions)\n",
    "        A list or array of points representing the contour. Each point should have the same dimensionality as `point`.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    closest_point : array-like\n",
    "        The point on the contour that is closest to the input `point`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build a KD-tree for fast nearest neighbor search over the contour points\n",
    "    tree = KDTree(contour)\n",
    "\n",
    "    # Find the index of the contour point closest to the input point\n",
    "    closest_index = tree.query(point)[1]\n",
    "\n",
    "    # If the result is an array (e.g., due to batch input), extract the scalar index\n",
    "    if not isinstance(closest_index, np.int64): \n",
    "        closest_index = closest_index[0]\n",
    "\n",
    "    # Retrieve the actual closest point using the index\n",
    "    closest_point = contour[closest_index]\n",
    "\n",
    "    return closest_point\n",
    "\n",
    "def closest_border_point(border_points, contour): \n",
    "    \"\"\"\n",
    "    Finds the point in `border_points` that is closest to any point in the given `contour`.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    border_points : array-like of shape (n_points, n_dimensions)\n",
    "        A list or array of candidate points (e.g., border or edge points).\n",
    "    \n",
    "    contour : array-like of shape (m_points, n_dimensions)\n",
    "        A list or array of contour points to which the closest distance is measured.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    min_point : array-like\n",
    "        The point from `border_points` that is closest to any point in the `contour`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build a KDTree for efficient nearest neighbor queries on contour points\n",
    "    tree = KDTree(contour)\n",
    "\n",
    "    # Initialize variables to track the closest border point and the smallest distance found\n",
    "    min_point = None         # Will hold the closest point from `border_points`\n",
    "    total_min = float('inf') # Initialize the minimum distance as infinity\n",
    "\n",
    "    # Iterate through each candidate border point\n",
    "    for border_point in border_points: \n",
    "        # Query the KDTree to find the distance to the closest contour point\n",
    "        dist, _ = tree.query(border_point)\n",
    "\n",
    "        # If this distance is the smallest encountered so far, update tracking variables\n",
    "        if dist < total_min: \n",
    "            total_min = dist \n",
    "            min_point = border_point \n",
    "    \n",
    "    # Return the border point with the minimum distance to the contour\n",
    "    return min_point\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    \"\"\"\n",
    "    Computes the Euclidean distance between two points.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    point1 : array-like\n",
    "        The first point (e.g., [x1, y1] or [x1, y1, z1]).\n",
    "    \n",
    "    point2 : array-like\n",
    "        The second point (e.g., [x2, y2] or [x2, y2, z2]).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        The Euclidean distance between `point1` and `point2`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert both points to NumPy arrays, subtract them element-wise,\n",
    "    # and compute the L2 norm (i.e., Euclidean distance) of the result\n",
    "    return np.linalg.norm(np.array(point1) - np.array(point2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49983339 0.49983339 0.49983339]\n",
      "[3 3 3]\n",
      "New point P with desired x1 movement: [2.49950017 3.49950017 4.49950017]\n"
     ]
    }
   ],
   "source": [
    "def move_from_A_to_B_with_x1_displacement(A, B, deltas, epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    Move from point A to point B in n-dimensional space with a desired movement in the x1 dimension.\n",
    "    \n",
    "    Parameters:\n",
    "    - A: list or np.array, coordinates of the starting point A\n",
    "    - B: list or np.array, coordinates of the target point B\n",
    "    - delta_x1: float, the desired displacement in the x1 dimension\n",
    "    \n",
    "    Returns:\n",
    "    - P: np.array, coordinates of the new point after moving delta_x1 along x1-axis\n",
    "    \"\"\"\n",
    "    A = np.array(A)\n",
    "    B = np.array(B)\n",
    "    \n",
    "    # Calculate direction vector from A to B\n",
    "    D = B - A\n",
    "    \n",
    "    # Calculate the scaling factor t for the desired movement in x1\n",
    "    t = deltas / (D + epsilon)   # D[0] is the x1 component of the direction vector\n",
    "    \n",
    "    # Calculate the new point P based on t\n",
    "    P = A + t * D\n",
    "\n",
    "    print(t) \n",
    "    print(D)\n",
    "    \n",
    "    return P\n",
    "\n",
    "# Example usage\n",
    "A = [1, 2, 3]  # Starting point in 3D space\n",
    "B = [4, 5, 6]  # Target point in 3D space\n",
    "delta_x1 = 1.5  # Desired movement in x1 dimension\n",
    "\n",
    "P = move_from_A_to_B_with_x1_displacement(A, B, delta_x1)\n",
    "print(\"New point P with desired x1 movement:\", P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def prediction(Z, grid, epsilon): \n",
    "    \"\"\"\n",
    "    Identify points near the decision boundary by finding close pairs of grid points\n",
    "    with different predicted classes and computing their midpoints.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Z : numpy.ndarray\n",
    "        Array of predicted class labels for each point in the grid (shape: (n_grid_points,)).\n",
    "    \n",
    "    grid : numpy.ndarray\n",
    "        Array of grid points in the feature space (shape: (n_grid_points, n_features)).\n",
    "    \n",
    "    epsilon : float\n",
    "        Maximum distance threshold for considering two points as neighbors. Points\n",
    "        closer than this with different classes are used to compute boundary midpoints.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of numpy.ndarray\n",
    "        List of midpoint arrays representing approximate boundary points.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    None explicitly, but may raise ValueError if shapes of Z and grid mismatch,\n",
    "    or if numba compilation fails (if @njit is used without numba installed).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This function is JIT-compiled with numba for performance, but can run without it.\n",
    "      Remove @njit if numba is unavailable.\n",
    "    - Computational complexity is O(n_grid_points^2), which is inefficient for large grids.\n",
    "      Suitable only for small grids (e.g., low resolution or few features).\n",
    "    - Uses Euclidean norm (np.linalg.norm) for distance.\n",
    "    - Usage: Called internally by boundary computation functions to extract transitions.\n",
    "    \"\"\"\n",
    "    boundary_points = []  # List to collect midpoints\n",
    "    for i in range(len(grid) - 1):  # Outer loop over grid points\n",
    "            for j in range(i + 1, len(grid)):  # Inner loop to avoid duplicates/self\n",
    "                # Check if points are close and have different predictions\n",
    "                if np.linalg.norm(grid[i] - grid[j]) < epsilon and Z[i] != Z[j]:\n",
    "                    # Append midpoint as boundary approximation\n",
    "                    boundary_points.append((grid[i] + grid[j]) / 2)  \n",
    "    return boundary_points\n",
    "\n",
    "def compute_decision_boundary_points_all_features(model, X, resolution=100, epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Compute decision boundary points in the high-dimensional feature space by\n",
    "    generating a dense grid, predicting classes, and finding transitions via midpoints.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        Trained binary or multi-class classifier with a `predict` method that takes\n",
    "        an array of input points and returns predictions as an array.\n",
    "        Example: sklearn.linear_model.LogisticRegression instance.\n",
    "    \n",
    "    X : pandas.DataFrame\n",
    "        Input feature dataset (shape: (n_samples, n_features)). Used to determine\n",
    "        min/max ranges for numeric features and categories for categorical ones.\n",
    "    \n",
    "    resolution : int, optional\n",
    "        Number of points to sample along each feature axis for the grid. Higher values\n",
    "        increase density but exponentially increase memory/computation. Default is 100.\n",
    "    \n",
    "    epsilon : float, optional\n",
    "        Distance threshold for detecting class changes between grid points. Should be\n",
    "        tuned based on feature scales (e.g., small for normalized data). Default is 0.01.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame of unique approximate boundary points, with the same columns as X.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If grid shapes mismatch during generation, or if model.predict fails.\n",
    "    \n",
    "    MemoryError or OverflowError\n",
    "        Likely for high resolution or many features due to grid size (resolution ** n_features).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Generates a full Cartesian grid over all features, flattened to 1D array.\n",
    "      For n_features = f, grid size = resolution^f, which is feasible only for small f\n",
    "      (e.g., f<=3) and low resolution (e.g., <=20). For higher dimensions, consider\n",
    "      sampling or dimensionality reduction instead.\n",
    "    - For numeric features: Samples evenly from (min-1, max+1).\n",
    "    - For categorical features: Maps to integer indices (0 to len(categories)-1).\n",
    "      Note: The code assumes len(categories) == resolution; otherwise, it may produce\n",
    "      an incorrect grid size (length mismatch). Consider adjusting resolution to match\n",
    "      max categories or handling categoricals separately (e.g., one-hot encode beforehand).\n",
    "    - Predictions are made on the entire grid, then boundary midpoints are found.\n",
    "    - Unique points are taken to remove duplicates.\n",
    "    - Usage: Visualize high-D boundaries by projecting (e.g., PCA) or for analysis.\n",
    "      Best for low-D or with small resolution.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> import pandas as pd\n",
    "    >>> from sklearn.linear_model import LogisticRegression\n",
    "    >>> X = pd.DataFrame({'feat1': [0, 1, 2], 'feat2': [0, 1, 0]})\n",
    "    >>> model = LogisticRegression().fit(X, [0, 1, 0])\n",
    "    >>> boundary = compute_decision_boundary_points_all_features(model, X, resolution=10, epsilon=0.1)\n",
    "    >>> print(boundary.shape)  # e.g., (number_of_boundary_points, 2)\n",
    "    (15, 2)\n",
    "    \"\"\"\n",
    "    n_features = X.shape[1]  # Number of features\n",
    "    # A grid that contains resolution^f samples from the f dimensional space where f is the number of features\n",
    "    grid = np.zeros((resolution ** n_features, n_features))  # Initialize grid array\n",
    "\n",
    "    # Generates a grid that contains resolution^f samples based on whether the column contains numeric or categorical values \n",
    "    # If the column contains numeric types, then the grid generates a column based on subdividing the numeric columns evenly\n",
    "    for i in range(n_features):\n",
    "        # Checks if the column is not a column consisting of categorical values\n",
    "        # If it is not categorical, then the column must be numeric. \n",
    "        if not isinstance(X[X.columns[i]].dtype, pd.CategoricalDtype):\n",
    "            # Sample evenly spaced points, slightly extended beyond data range\n",
    "            grid[:, i] = np.tile(np.linspace(X.iloc[:, i].min() - 1, X.iloc[:, i].max() + 1, resolution).repeat(resolution ** (n_features - i - 1)), resolution ** i)\n",
    "        else:\n",
    "            # For categorical: Get unique categories and map to integers\n",
    "            cat_array = X.iloc[:, i].astype('category').cat.categories\n",
    "            cat_array = np.arange(len(cat_array))  # e.g., [0, 1, 2] for 3 categories\n",
    "            repeats_per_cat = resolution ** (n_features - i - 1)\n",
    "            tiles = resolution ** i\n",
    "            col_values = np.tile(np.repeat(cat_array, repeats_per_cat), tiles)\n",
    "            grid[:, i] = col_values  # Assign to grid; note: length must match resolution^n_features\n",
    "            \n",
    "    # Predict the class for each point in the grid\n",
    "    Z = model.predict(grid)\n",
    "    # Find points near the decision boundary\n",
    "    boundary_points = prediction(Z, grid, epsilon)\n",
    " \n",
    "    return pd.DataFrame(np.unique(boundary_points,axis=0), columns=X.columns)  # Unique points as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_multi_dim_border_points(center, extents, step=0.1):\n",
    "    \"\"\"\n",
    "    Generate points on the boundaries of an n-dimensional hyperrectangle.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    center : list or numpy.ndarray\n",
    "        The center of the hyperrectangle, a list or array of length n (number of dimensions).\n",
    "    \n",
    "    extents : list or numpy.ndarray\n",
    "        The full widths (diameters) in each dimension, a list or array of length n.\n",
    "        Note: The code uses half-widths internally (extents / 2).\n",
    "    \n",
    "    step : float, optional\n",
    "        Step size for sampling points along each dimension's grid. Smaller values\n",
    "        increase density but computation time. Default is 0.1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of tuples\n",
    "        Each tuple represents a point on the boundary of the hyperrectangle.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    None explicitly, but may raise ValueError if center and extents have mismatched lengths,\n",
    "    or TypeError if inputs are not array-like.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Uses a set to avoid duplicate points, which can occur at corners/edges.\n",
    "    - For each dimension, fixes the boundary (min/max) and grids over others.\n",
    "    - Handles 1D case specially.\n",
    "    - Suitable for generating boundary samples in constrained optimization or\n",
    "      visualization of feasible regions in n-D space.\n",
    "    - Output as list of tuples for easy conversion to arrays if needed.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> center = [0, 0]\n",
    "    >>> extents = [2, 2]  # Rectangle from (-1,-1) to (1,1)\n",
    "    >>> points = get_multi_dim_border_points(center, extents, step=0.5)\n",
    "    >>> print(len(points))  # e.g., number of sampled boundary points\n",
    "    16\n",
    "    \"\"\"\n",
    "    center = np.array(center)  # Convert center to NumPy array\n",
    "    extents = np.array(extents)  # Convert extents to NumPy array\n",
    "    n = len(center)  # Number of dimensions\n",
    "    points = set()   # Use set to avoid duplicates\n",
    "    \n",
    "    # Define min and max bounds for each dimension (using half-extents)\n",
    "    bounds = [(c - e / 2, c + e / 2) for c, e in zip(center, extents)]\n",
    "    \n",
    "    # For each dimension, generate points on the lower and upper boundaries\n",
    "    for dim in range(n):\n",
    "        # For lower and upper boundary in this dimension\n",
    "        for bound_val in [bounds[dim][0], bounds[dim][1]]:\n",
    "            # Generate grid points for all other dimensions\n",
    "            other_dims = [i for i in range(n) if i != dim]\n",
    "            ranges = [np.arange(bounds[i][0], bounds[i][1] + step, step) for i in other_dims]\n",
    "            if not ranges:  # Handle 1D case\n",
    "                points.add(tuple([bound_val] if dim == 0 else []))\n",
    "                continue\n",
    "            # Create meshgrid for other dimensions\n",
    "            grids = np.meshgrid(*ranges, indexing='ij')\n",
    "            coords = [grid.ravel() for grid in grids]\n",
    "            \n",
    "            # Construct points\n",
    "            for coord in zip(*coords):\n",
    "                point = [0] * n\n",
    "                # Set the current dimension to the boundary value\n",
    "                point[dim] = bound_val\n",
    "                # Set other dimensions to the grid values\n",
    "                for i, val in zip(other_dims, coord):\n",
    "                    point[i] = val\n",
    "                points.add(tuple(point))  # Add as tuple to set\n",
    "    \n",
    "    return list(points)\n",
    "\n",
    "def det_constraints(datapt, deltas): \n",
    "    \"\"\"\n",
    "    Determine the effective constraints based on deltas, scaling them relative to the data point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datapt : list or numpy.ndarray\n",
    "        The data point (feature vector) to scale constraints against.\n",
    "    \n",
    "    deltas : list\n",
    "        List of delta values for each feature. If float/int, it's treated as a percentage\n",
    "        (e.g., 10 means 10% of datapt[i]); otherwise ignored.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (constraints: list of scaled delta values or -1 if not applicable,\n",
    "         len_constr: int count of active constraints)\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    None explicitly, but may raise TypeError if datapt/deltas are incompatible.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Initializes constraints as [-1] * len(deltas), updating only for numeric deltas.\n",
    "    - Scaling: constraint[i] = (deltas[i] / 100) * datapt[i], assuming percentages.\n",
    "    - Used to count and quantify constraints for bounded regions.\n",
    "    - Usage: Pre-process deltas before applying bounds in optimization or counterfactuals.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> datapt = [100, 200]\n",
    "    >>> deltas = [10, 'none']  # 10% for first, ignore second\n",
    "    >>> constraints, len_constr = det_constraints(datapt, deltas)\n",
    "    >>> print(constraints, len_constr)  # [10.0, -1], 1\n",
    "    [10.0, -1] 1\n",
    "    \"\"\"\n",
    "    constraints = [-1] * len(deltas)  # Initialize with -1 (inactive)\n",
    "    len_constr = 0  # Counter for active constraints\n",
    "    for i in range(len(deltas)): \n",
    "        if type(deltas[i]) == float or type(deltas[i]) == int:  # Check if numeric\n",
    "            constraints[i] = (deltas[i]/100)*datapt[i]  # Scale as percentage of datapt\n",
    "            len_constr+=1  # Increment counter\n",
    "    return constraints, len_constr\n",
    "\n",
    "def constraint_bounds(contours, datapt, constraints): \n",
    "    \"\"\"\n",
    "    Filter contour points to those within specified bounds based on constraints.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    contours : numpy.ndarray\n",
    "        Array of contour points (shape: (n_points, n_features)).\n",
    "    \n",
    "    datapt : numpy.ndarray\n",
    "        The reference data point (shape: (1, n_features)) to center bounds around.\n",
    "    \n",
    "    constraints : list\n",
    "        List of delta values (bounds widths) for each feature; >0 activates filtering.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Filtered contour points within the bounds.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    Exception\n",
    "        If no constraints are assigned (all <=0).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - For each active constraint, computes [x - delta/2, x + delta/2] and filters.\n",
    "    - Sequentially applies filters, potentially reducing points cumulatively.\n",
    "    - Includes plotting of bounds (vertical/horizontal lines for dims 0/1).\n",
    "    - Assumes 2D for plotting; extend for higher dims if needed.\n",
    "    - Usage: Constrain boundary points in optimization, e.g., feasible counterfactuals.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> contours = np.array([[0,0], [1,1], [2,2], [3,3]])\n",
    "    >>> datapt = np.array([[1,1]])\n",
    "    >>> constraints = [2, 2]  # Bounds width 2 for each\n",
    "    >>> bounded = constraint_bounds(contours, datapt, constraints)\n",
    "    >>> print(bounded)  # e.g., array([[0,0], [1,1], [2,2]])\n",
    "    \"\"\"\n",
    "    if len(constraints) == 0: \n",
    "        raise Exception(\"No constraints were assigned.\")\n",
    "    bounded_contour = contours.copy()  # Copy to avoid modifying original\n",
    "    for i in range(len(constraints)): \n",
    "        if constraints[i] > 0:  # Active if >0\n",
    "            x = datapt[0][i]  # Reference value for dimension i\n",
    "            # This should just be the delta\n",
    "            delta_x = constraints[i]\n",
    "            # Generate a lower and upper bounds on each constraint\n",
    "            lowb_x, highb_x = x - (delta_x / 2), x + (delta_x / 2)\n",
    "            contour_arr = bounded_contour[:, i]  # Extract column i\n",
    "            # Choose the correct indices for the multi-dimensional df\n",
    "            indices = np.where((contour_arr >= lowb_x) & (contour_arr <= highb_x))\n",
    "            bounded_contour_pts = bounded_contour[indices]  # Filter rows\n",
    "            bounded_contour = bounded_contour_pts  # Update\n",
    "            if i == 0:  # Plot vertical lines for dim 0\n",
    "                plt.axvline(x=highb_x, color='b', linestyle='-', label='High Bound x')\n",
    "                plt.axvline(x=lowb_x, color='b', linestyle='-', label='Low bound x')\n",
    "            else:   # Plot horizontal for dim 1 (assumes 2D)\n",
    "                plt.axhline(y=highb_x, color='b', linestyle='-', label='High Bound y')\n",
    "                plt.axhline(y=lowb_x, color='b', linestyle='-', label='Low bound y')\n",
    "    return bounded_contour\n",
    "\n",
    "def real_world_constraints(points, undesired_coords, constraints): \n",
    "    \"\"\"\n",
    "    Filter points based on real-world constraints relative to undesired_coords.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    points : pandas.DataFrame\n",
    "        DataFrame of points to filter, with feature columns.\n",
    "    \n",
    "    undesired_coords : list or array\n",
    "        The reference (\"undesired\") point coordinates, matching points' features.\n",
    "    \n",
    "    constraints : list of lists, optional\n",
    "        Each sublist: [feature_name (str), operator ('equal', 'greater', or other for <)].\n",
    "        Empty list returns points unchanged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Filtered points satisfying all constraints.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    None explicitly, but may raise KeyError if feature_name not in points.columns,\n",
    "    or IndexError if constraints malformed.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Sequentially applies filters: == for 'equal', > for 'greater', < otherwise.\n",
    "    - Uses column index from get_loc for comparison value.\n",
    "    - Useful for imposing domain-specific rules, e.g., in counterfactual explanations.\n",
    "    - If constraints empty, returns original points.\n",
    "    - Usage: Post-process boundary points to respect real-world feasibility.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> points = pd.DataFrame({'pt1': [1,2,3], 'pt2': [4,5,6], 'pt3': [10,11,12]}, columns=['feat1','feat2','feat3'])\n",
    "    >>> undesired_coords = [1,2,5]\n",
    "    >>> constraints = [['feat1', 'greater']]\n",
    "    >>> filtered = real_world_constraints(points, undesired_coords, constraints)\n",
    "    >>> print(filtered)  # pd.DataFrame(data=[[4,5,6], [10,11,12]]) # Since feat1 = 1 for the undesired point, we select all points that have feat1 > 1\n",
    "    >>> constraints = [['feat1', 'greater'], ['feat2', 'less']]. \n",
    "    >>> filtered = real_world_constraints(points, undesired_coords, constraints)\n",
    "    >>> print(filtered)  # pd.DataFrame(data=[]) # Since feat2 = 2 for the undesired point, we select all points that have feat2 < 2. \n",
    "    \"\"\"\n",
    "    if len(constraints) == 0: \n",
    "        return points \n",
    "    \n",
    "    for constraint in constraints: \n",
    "        select_pts = None\n",
    "        if constraint[1] == \"equal\":  # Filter equal to undesired value\n",
    "            select_pts = points.loc[points[constraint[0]] == undesired_coords[points.columns.get_loc(constraint[0])], :]\n",
    "        elif constraint[1] == \"greater\":  # Filter greater than\n",
    "            select_pts = points.loc[points[constraint[0]] > undesired_coords[points.columns.get_loc(constraint[0])], :] \n",
    "        else:  # Default: less than\n",
    "            select_pts = points.loc[points[constraint[0]] < undesired_coords[points.columns.get_loc(constraint[0])], :]\n",
    "\n",
    "        points = select_pts  # Update with filtered\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_point(dataset, model, desired_class, original_class, undesired_coords, resolution=100, point_epsilon=0.1, epsilon=0.01, constraints=[], deltas=[]): \n",
    "    \"\"\"\n",
    "    Finds the closest point to the decision boundary from an undesired point using a grid-based\n",
    "    approximation of the boundary, optionally constrained by real-world conditions. This generates\n",
    "    a counterfactual explanation by minimizing the distance to the boundary while satisfying class\n",
    "    change requirements and constraints.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pd.DataFrame\n",
    "        Full dataset containing features and a final column with class labels.\n",
    "    \n",
    "    model : sklearn-like classifier\n",
    "        A binary classification model with a `.fit()` and `.predict()` method.\n",
    "    \n",
    "    desired_class : int or label\n",
    "        The target class we want the corrected point to belong to.\n",
    "    \n",
    "    original_class : int or label\n",
    "        The actual class label of the undesired point.\n",
    "    \n",
    "    undesired_coords : list or array\n",
    "        The coordinates of the original (\"unhealthy\") point.\n",
    "    \n",
    "    resolution : int, optional\n",
    "        Number of points to sample along each feature axis for the grid in boundary computation.\n",
    "        Higher values improve accuracy but increase computation exponentially. Default is 100.\n",
    "    \n",
    "    point_epsilon : float, optional\n",
    "        Distance threshold for detecting class changes in the grid-based boundary search.\n",
    "        Default is 0.1.\n",
    "    \n",
    "    epsilon : float, optional\n",
    "        Step size used when displacing a point toward the decision boundary (for overshooting).\n",
    "        Default is 0.01.\n",
    "    \n",
    "    constraints : list, optional\n",
    "        A list of real-world constraints on the features (e.g., ranges, logic constraints).\n",
    "        Default is [].\n",
    "    \n",
    "    deltas : list, optional\n",
    "        Tolerances or maximum displacements for each feature. Default is [].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        A corrected point (optimal_datapt) that satisfies the class change and real-world constraints.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    Exception\n",
    "        If the number of constraints exceeds the number of features.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This variant uses a grid-based approach (`compute_decision_boundary_points_all_features`)\n",
    "      to approximate the decision boundary, which is more exhaustive but computationally intensive\n",
    "      for high dimensions or resolutions. Suitable for low-dimensional spaces (e.g., 2-3 features).\n",
    "    - Trains the model on the dataset, generates boundary points via grid predictions and midpoint\n",
    "      detection, applies constraints, and finds the closest optimal point.\n",
    "    - Assumes binary classification and relies on external functions like `real_world_constraints`,\n",
    "      `closest_point`, `move_from_A_to_B_with_x1_displacement`, `det_constraints`,\n",
    "      `get_multi_dim_border_points`, `constraint_bounds`, and `closest_border_point`,\n",
    "      which must be defined elsewhere.\n",
    "    - Includes plotting for visualization (e.g., contours, points, lines), requiring matplotlib.\n",
    "      Plots assume 2D for simplicity (e.g., contours[:,0] and [:,1]).\n",
    "    - Print statements provide progress tracking.\n",
    "    - If `desired_class != original_class`, overshoots the boundary slightly for class flip.\n",
    "      Otherwise, handles bounded constraints differently (full grid or partial filtering).\n",
    "    - Usage: Generate counterfactuals for explainable AI, optimization, or model interpretation.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> from sklearn.svm import SVC\n",
    "    >>> dataset = pd.DataFrame({'feat1': [0, 1, 2], 'feat2': [0, 1, 0], 'label': [0, 1, 0]})\n",
    "    >>> model = SVC(kernel='linear')\n",
    "    >>> undesired_coords = [2, 0]  # Example point from class 0\n",
    "    >>> optimal = optimal_point(dataset, model, desired_class=1, original_class=0, undesired_coords=undesired_coords, resolution=20)\n",
    "    >>> print(optimal)  # e.g., array([[1.5, 0.5]])\n",
    "    \"\"\"\n",
    "    X_train, y_train = dataset.iloc[:, 0:dataset.shape[1]-1], dataset.iloc[:, -1]  # Extract features and labels\n",
    "    n_features = X_train.shape[1]  # Get number of features\n",
    "\n",
    "    print(\"fitting model...\")\n",
    "    model.fit(X_train, y_train)  # Train the model\n",
    "    print(\"model finished.\")\n",
    "\n",
    "    print(\"boundary points started generation...\")\n",
    "    # Use grid-based method to approximate boundary points\n",
    "    boundary_points = compute_decision_boundary_points_all_features(model, X_train, resolution=resolution, epsilon=point_epsilon)\n",
    "    print(\"boundary points finished.\")\n",
    "\n",
    "    # Fitting the boundary points to the constraints provided by the real world\n",
    "    contours = real_world_constraints(points=boundary_points, undesired_coords=undesired_coords, constraints=constraints)\n",
    "    contours = contours.to_numpy()  # Convert to NumPy for further processing\n",
    "\n",
    "    # contours = boundary_points  # (Commented: Alternative to use raw boundary)\n",
    "    undesired_datapt = np.reshape(np.array(list(undesired_coords)), (1, -1))  # Reshape undesired point to 2D array\n",
    "    # Find the closest point from the undesired point to the contour line\n",
    "    print(\"Finding the closest point from the contour line to the point...\")\n",
    "    optimal_datapt = closest_point(undesired_datapt, contour=contours)\n",
    "    print(\"Finding the closest point from the contour line to the point.\")  # Note: Duplicate print, possibly a typo\n",
    "    plt.plot(contours[:,0], contours[:,1], lw=0.5, color='red')  # Plot contours (assumes 2D)\n",
    "\n",
    "    if desired_class != original_class: \n",
    "        D = optimal_datapt - undesired_datapt  # Direction vector to boundary\n",
    "        deltas = D * (1+epsilon)  # Scale to overshoot slightly\n",
    "        optimal_datapt = move_from_A_to_B_with_x1_displacement(undesired_datapt, optimal_datapt, deltas=deltas)  # Move point\n",
    "    else: \n",
    "        closest_boundedpt = None\n",
    "        deltas, len_constr = det_constraints(datapt=undesired_datapt[0], changes=deltas)  # Determine constraints (note: param 'changes' may be a typo for 'deltas')\n",
    "        bounded_contour_pts = None\n",
    "\n",
    "        if len_constr > n_features: \n",
    "            raise Exception(\"There cannot be more constraints than features\")\n",
    "        elif len_constr == n_features:\n",
    "            # Generate border points for fully constrained case\n",
    "            bounded_contour_pts = get_multi_dim_border_points(center=undesired_datapt[0], extents=deltas, step=0.05)\n",
    "            np_bounded_contour = np.array(bounded_contour_pts)  # To NumPy\n",
    "            x_values, y_values = np_bounded_contour[:,0], np_bounded_contour[:, 1]  # Extract for plotting (assumes 2D)\n",
    "            plt.scatter(x_values, y_values, marker='o')  # Plot bounded points\n",
    "            closest_boundedpt = closest_border_point(bounded_contour_pts, contour=contours)  # Find closest on border (constraints in all dimensions)\n",
    "        else: \n",
    "            # Generate bounded contour points for partial constraints \n",
    "            bounded_contour_pts = constraint_bounds(contours, undesired_datapt, deltas)\n",
    "            closest_boundedpt = closest_point(point=undesired_datapt, contour=bounded_contour_pts)  # Find closest point based on partial constraints\n",
    "\n",
    "        D = closest_boundedpt - undesired_datapt  # Direction vector\n",
    "        optimal_datapt = move_from_A_to_B_with_x1_displacement(undesired_datapt, closest_boundedpt, deltas=D)  # Move point\n",
    "    plt.scatter(undesired_datapt[0][0], undesired_datapt[0][1], c = 'r')  # Plot undesired point\n",
    "    plt.text(undesired_datapt[0][0]+0.002, undesired_datapt[0][1]+0.002, 'NH')  # Label 'NH' (e.g., Non-Healthy)\n",
    "    plt.scatter(optimal_datapt[0][0], optimal_datapt[0][1], c = 'r')  # Plot optimal point\n",
    "    plt.text(optimal_datapt[0][0]+0.002, optimal_datapt[0][1]+0.002, 'NH')  # Label 'NH' (note: duplicate label, perhaps typo for 'H')\n",
    "    plt.plot([undesired_datapt[0][0], optimal_datapt[0][0]], [undesired_datapt[0][1],optimal_datapt[0][1]], linestyle='--')  # Dashed line between points\n",
    "    return optimal_datapt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments of the Computation Costs for Grid-based Approach (Logistic Regression with Numba)\n",
    "\n",
    "$\\textbf{ Logistic Regression with Grid-based Method }$ -- 50 features\n",
    "\n",
    "Resolution: $R = 15$ ($15^{50}$ points), Memory Error (Maximum allowed dimension exceeded), 0 boundary points found\n",
    "\n",
    "Resolution: $R = 10$ ($10^{50}$ points), Memory Error (Maximum allowed dimension exceeded), 0 boundary points found\n",
    "\n",
    "$\\textbf{ Logistic Regression with Grid-based Method }$ -- 10 features\n",
    "\n",
    "Resolution: $R = 15$ ($15^{15}$ points), 42 Terabyte Memory Error, 0 boundary points found\n",
    "\n",
    "Resolution: $R = 10$ ($10^{10}$ points), 745 Gigabytes Memory Error, 0 boundary points found\n",
    "\n",
    "\n",
    "$\\textbf{ Logistic Regression with Grid-based Method }$ -- 2 features\n",
    "\n",
    "Resolution: $R = 150$ ($150^2 = 22,500$ points searched), 27.2 second runtime, 454 boundary points found\n",
    "\n",
    "Resolution: $R = 100$ ($100^2 = 10,000$ points searched) 5.4 seconds runtime, 104 boundary points found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=2000, n_features=2, n_informative=2, n_redundant=0, random_state=42, n_classes=2)\n",
    "model = LogisticRegression()\n",
    "y = y.reshape(-1,1)\n",
    "df1 = pd.DataFrame(data=np.hstack((X,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.800062</td>\n",
       "      <td>-0.957489</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.187099</td>\n",
       "      <td>1.159787</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.154512</td>\n",
       "      <td>1.217520</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.179014</td>\n",
       "      <td>-0.852832</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.735827</td>\n",
       "      <td>-0.245366</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.039487</td>\n",
       "      <td>1.320957</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.482199</td>\n",
       "      <td>0.419738</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.622829</td>\n",
       "      <td>-0.803223</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.965721</td>\n",
       "      <td>-1.068587</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.798459</td>\n",
       "      <td>-1.022348</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1    2\n",
       "0  0.800062 -0.957489  1.0\n",
       "1  1.187099  1.159787  1.0\n",
       "2  0.154512  1.217520  0.0\n",
       "3  0.179014 -0.852832  1.0\n",
       "4 -0.735827 -0.245366  0.0\n",
       "5  0.039487  1.320957  1.0\n",
       "6 -1.482199  0.419738  0.0\n",
       "7 -0.622829 -0.803223  0.0\n",
       "8  0.965721 -1.068587  1.0\n",
       "9  0.798459 -1.022348  1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model...\n",
      "model finished.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "Finding the closest point from the contour line to the point...\n",
      "Finding the closest point from the contour line to the point.\n",
      "[[1.07092306 0.87624128]]\n",
      "[[-1.16018181  0.00452233]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.05536678,  1.16374982]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_point(df1, model=model, desired_class=0, original_class=1, resolution=150, undesired_coords=df1.iloc[1,:df1.shape[1]-1], point_epsilon=0.1, epsilon=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model...\n",
      "model finished.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "Finding the closest point from the contour line to the point...\n",
      "Finding the closest point from the contour line to the point.\n",
      "[[1.07092411 1.09801295]]\n",
      "[[-1.15887253 -0.03919662]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.05396586,  1.11674877]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_point(df1, model=model, desired_class=0, original_class=1, resolution=100, undesired_coords=df1.iloc[1,:df1.shape[1]-1], point_epsilon=0.1, epsilon=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=2000, n_features=10, n_informative=10, n_redundant=0, random_state=42, n_classes=2)\n",
    "model = LogisticRegression()\n",
    "y = y.reshape(-1,1)\n",
    "df2 = pd.DataFrame(data=np.hstack((X,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.990620</td>\n",
       "      <td>-2.921959</td>\n",
       "      <td>-5.689956</td>\n",
       "      <td>3.173745</td>\n",
       "      <td>-4.393239</td>\n",
       "      <td>0.538239</td>\n",
       "      <td>-0.962650</td>\n",
       "      <td>-3.024594</td>\n",
       "      <td>-0.837913</td>\n",
       "      <td>-1.944185</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.342356</td>\n",
       "      <td>0.942957</td>\n",
       "      <td>3.057782</td>\n",
       "      <td>0.059667</td>\n",
       "      <td>-0.611317</td>\n",
       "      <td>-0.985521</td>\n",
       "      <td>0.323851</td>\n",
       "      <td>0.853719</td>\n",
       "      <td>1.741036</td>\n",
       "      <td>2.069323</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.063987</td>\n",
       "      <td>2.185873</td>\n",
       "      <td>-2.230002</td>\n",
       "      <td>1.095676</td>\n",
       "      <td>-0.240747</td>\n",
       "      <td>-1.776678</td>\n",
       "      <td>-4.165090</td>\n",
       "      <td>-0.122159</td>\n",
       "      <td>0.688403</td>\n",
       "      <td>1.401783</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.852159</td>\n",
       "      <td>1.297380</td>\n",
       "      <td>3.466596</td>\n",
       "      <td>4.114327</td>\n",
       "      <td>-4.004815</td>\n",
       "      <td>-0.591707</td>\n",
       "      <td>2.035825</td>\n",
       "      <td>3.118117</td>\n",
       "      <td>-0.996438</td>\n",
       "      <td>1.465570</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.033975</td>\n",
       "      <td>-0.181000</td>\n",
       "      <td>0.438378</td>\n",
       "      <td>-1.506810</td>\n",
       "      <td>-0.503984</td>\n",
       "      <td>2.442108</td>\n",
       "      <td>-0.408010</td>\n",
       "      <td>1.722336</td>\n",
       "      <td>2.563095</td>\n",
       "      <td>-3.228898</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.770680</td>\n",
       "      <td>-0.667671</td>\n",
       "      <td>-2.726522</td>\n",
       "      <td>1.778277</td>\n",
       "      <td>3.033320</td>\n",
       "      <td>1.052492</td>\n",
       "      <td>1.810689</td>\n",
       "      <td>-3.730430</td>\n",
       "      <td>-2.032935</td>\n",
       "      <td>2.371080</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.375939</td>\n",
       "      <td>0.889744</td>\n",
       "      <td>-0.487178</td>\n",
       "      <td>0.394568</td>\n",
       "      <td>-1.553368</td>\n",
       "      <td>2.302102</td>\n",
       "      <td>-0.134183</td>\n",
       "      <td>1.340100</td>\n",
       "      <td>0.741324</td>\n",
       "      <td>0.554025</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.060123</td>\n",
       "      <td>-2.850018</td>\n",
       "      <td>-1.808885</td>\n",
       "      <td>1.649850</td>\n",
       "      <td>0.827631</td>\n",
       "      <td>-0.681723</td>\n",
       "      <td>3.049334</td>\n",
       "      <td>-1.560887</td>\n",
       "      <td>-0.029848</td>\n",
       "      <td>0.037102</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.735121</td>\n",
       "      <td>4.266722</td>\n",
       "      <td>0.767749</td>\n",
       "      <td>-2.012636</td>\n",
       "      <td>2.538945</td>\n",
       "      <td>-4.960979</td>\n",
       "      <td>2.908603</td>\n",
       "      <td>-6.451668</td>\n",
       "      <td>2.723697</td>\n",
       "      <td>0.785976</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.436313</td>\n",
       "      <td>1.826231</td>\n",
       "      <td>-0.232626</td>\n",
       "      <td>-0.570040</td>\n",
       "      <td>1.101895</td>\n",
       "      <td>0.658344</td>\n",
       "      <td>-0.079847</td>\n",
       "      <td>-2.158960</td>\n",
       "      <td>0.985347</td>\n",
       "      <td>0.426746</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  1.990620 -2.921959 -5.689956  3.173745 -4.393239  0.538239 -0.962650   \n",
       "1  1.342356  0.942957  3.057782  0.059667 -0.611317 -0.985521  0.323851   \n",
       "2 -1.063987  2.185873 -2.230002  1.095676 -0.240747 -1.776678 -4.165090   \n",
       "3 -4.852159  1.297380  3.466596  4.114327 -4.004815 -0.591707  2.035825   \n",
       "4  0.033975 -0.181000  0.438378 -1.506810 -0.503984  2.442108 -0.408010   \n",
       "5  2.770680 -0.667671 -2.726522  1.778277  3.033320  1.052492  1.810689   \n",
       "6 -1.375939  0.889744 -0.487178  0.394568 -1.553368  2.302102 -0.134183   \n",
       "7  0.060123 -2.850018 -1.808885  1.649850  0.827631 -0.681723  3.049334   \n",
       "8  1.735121  4.266722  0.767749 -2.012636  2.538945 -4.960979  2.908603   \n",
       "9  0.436313  1.826231 -0.232626 -0.570040  1.101895  0.658344 -0.079847   \n",
       "\n",
       "         7         8         9    10  \n",
       "0 -3.024594 -0.837913 -1.944185  1.0  \n",
       "1  0.853719  1.741036  2.069323  1.0  \n",
       "2 -0.122159  0.688403  1.401783  1.0  \n",
       "3  3.118117 -0.996438  1.465570  1.0  \n",
       "4  1.722336  2.563095 -3.228898  1.0  \n",
       "5 -3.730430 -2.032935  2.371080  0.0  \n",
       "6  1.340100  0.741324  0.554025  1.0  \n",
       "7 -1.560887 -0.029848  0.037102  0.0  \n",
       "8 -6.451668  2.723697  0.785976  0.0  \n",
       "9 -2.158960  0.985347  0.426746  0.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model...\n",
      "model finished.\n",
      "boundary points started generation...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 42.0 TiB for an array with shape (576650390625, 10) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m optimal_point(df2, model\u001b[38;5;241m=\u001b[39mmodel, desired_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, original_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, resolution\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, undesired_coords\u001b[38;5;241m=\u001b[39mdf2\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m,:df2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], point_epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.07\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m, in \u001b[0;36moptimal_point\u001b[1;34m(dataset, model, desired_class, original_class, undesired_coords, resolution, point_epsilon, epsilon, constraints, deltas)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboundary points started generation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m boundary_points \u001b[38;5;241m=\u001b[39m compute_decision_boundary_points_all_features(model, X_train, resolution\u001b[38;5;241m=\u001b[39mresolution, epsilon\u001b[38;5;241m=\u001b[39mpoint_epsilon)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboundary points finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Fitting the boundary points to the constraints provided by the real world\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 23\u001b[0m, in \u001b[0;36mcompute_decision_boundary_points_all_features\u001b[1;34m(model, X, resolution, epsilon)\u001b[0m\n\u001b[0;32m     21\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# A grid that contains R^f samples from the f dimensional space where f is the number of features\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m grid \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((resolution \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m n_features, n_features))\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Generates a grid that contains R^f samples based on whether the column contains numeric or categorical values \u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# If the column contains numeric types, then the grid generates a column based on subdividing the numeric columns evenly\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_features):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Checks if the column is not a column consisting of categorical values\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# If it is not categorical, then the column must be numeric. \u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 42.0 TiB for an array with shape (576650390625, 10) and data type float64"
     ]
    }
   ],
   "source": [
    "optimal_point(df2, model=model, desired_class=0, original_class=1, resolution=15, undesired_coords=df2.iloc[1,:df2.shape[1]-1], point_epsilon=0.1, epsilon=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model...\n",
      "model finished.\n",
      "boundary points started generation...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 745. GiB for an array with shape (10000000000, 10) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m optimal_point(df2, model\u001b[38;5;241m=\u001b[39mmodel, desired_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, original_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, resolution\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, undesired_coords\u001b[38;5;241m=\u001b[39mdf2\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m,:df2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], point_epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.07\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m, in \u001b[0;36moptimal_point\u001b[1;34m(dataset, model, desired_class, original_class, undesired_coords, resolution, point_epsilon, epsilon, constraints, deltas)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboundary points started generation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m boundary_points \u001b[38;5;241m=\u001b[39m compute_decision_boundary_points_all_features(model, X_train, resolution\u001b[38;5;241m=\u001b[39mresolution, epsilon\u001b[38;5;241m=\u001b[39mpoint_epsilon)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboundary points finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Fitting the boundary points to the constraints provided by the real world\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 23\u001b[0m, in \u001b[0;36mcompute_decision_boundary_points_all_features\u001b[1;34m(model, X, resolution, epsilon)\u001b[0m\n\u001b[0;32m     21\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# A grid that contains R^f samples from the f dimensional space where f is the number of features\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m grid \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((resolution \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m n_features, n_features))\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Generates a grid that contains R^f samples based on whether the column contains numeric or categorical values \u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# If the column contains numeric types, then the grid generates a column based on subdividing the numeric columns evenly\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_features):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Checks if the column is not a column consisting of categorical values\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# If it is not categorical, then the column must be numeric. \u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 745. GiB for an array with shape (10000000000, 10) and data type float64"
     ]
    }
   ],
   "source": [
    "optimal_point(df2, model=model, desired_class=0, original_class=1, resolution=10, undesired_coords=df2.iloc[1,:df2.shape[1]-1], point_epsilon=0.1, epsilon=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=2000, n_features=50, n_informative=50, n_redundant=0, random_state=42, n_classes=2)\n",
    "model = LogisticRegression()\n",
    "y = y.reshape(-1,1)\n",
    "df3 = pd.DataFrame(data=np.hstack((X,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.498960</td>\n",
       "      <td>1.740244</td>\n",
       "      <td>2.852685</td>\n",
       "      <td>0.483250</td>\n",
       "      <td>2.629995</td>\n",
       "      <td>2.421617</td>\n",
       "      <td>-0.605894</td>\n",
       "      <td>3.277559</td>\n",
       "      <td>-4.254546</td>\n",
       "      <td>0.657476</td>\n",
       "      <td>...</td>\n",
       "      <td>1.655511</td>\n",
       "      <td>-5.087583</td>\n",
       "      <td>-1.446621</td>\n",
       "      <td>5.061929</td>\n",
       "      <td>-4.813585</td>\n",
       "      <td>-8.186449</td>\n",
       "      <td>3.429404</td>\n",
       "      <td>-10.006712</td>\n",
       "      <td>3.805926</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.043912</td>\n",
       "      <td>1.305697</td>\n",
       "      <td>-4.162067</td>\n",
       "      <td>7.062152</td>\n",
       "      <td>10.188181</td>\n",
       "      <td>-5.216963</td>\n",
       "      <td>0.869844</td>\n",
       "      <td>6.395281</td>\n",
       "      <td>-14.499055</td>\n",
       "      <td>-2.626266</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.371640</td>\n",
       "      <td>-2.518497</td>\n",
       "      <td>-1.723985</td>\n",
       "      <td>-2.125003</td>\n",
       "      <td>-2.793473</td>\n",
       "      <td>-0.928793</td>\n",
       "      <td>-7.307003</td>\n",
       "      <td>-9.042577</td>\n",
       "      <td>-1.534964</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.699169</td>\n",
       "      <td>-8.643500</td>\n",
       "      <td>5.242382</td>\n",
       "      <td>4.438892</td>\n",
       "      <td>-0.622756</td>\n",
       "      <td>0.935984</td>\n",
       "      <td>5.435414</td>\n",
       "      <td>3.986439</td>\n",
       "      <td>5.502292</td>\n",
       "      <td>2.903077</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.215406</td>\n",
       "      <td>5.446412</td>\n",
       "      <td>4.758631</td>\n",
       "      <td>1.948837</td>\n",
       "      <td>-2.243126</td>\n",
       "      <td>-10.913266</td>\n",
       "      <td>2.709645</td>\n",
       "      <td>-5.339160</td>\n",
       "      <td>-5.383915</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.454609</td>\n",
       "      <td>4.730429</td>\n",
       "      <td>-1.116898</td>\n",
       "      <td>-5.399067</td>\n",
       "      <td>-1.189977</td>\n",
       "      <td>-1.699942</td>\n",
       "      <td>3.976442</td>\n",
       "      <td>4.936787</td>\n",
       "      <td>8.205961</td>\n",
       "      <td>-0.676943</td>\n",
       "      <td>...</td>\n",
       "      <td>1.208745</td>\n",
       "      <td>-1.076917</td>\n",
       "      <td>-1.119831</td>\n",
       "      <td>-0.596468</td>\n",
       "      <td>3.882105</td>\n",
       "      <td>4.146733</td>\n",
       "      <td>5.821566</td>\n",
       "      <td>5.414165</td>\n",
       "      <td>1.990073</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.400635</td>\n",
       "      <td>-2.403447</td>\n",
       "      <td>0.837233</td>\n",
       "      <td>1.377394</td>\n",
       "      <td>4.496939</td>\n",
       "      <td>-3.341007</td>\n",
       "      <td>0.073394</td>\n",
       "      <td>-1.536370</td>\n",
       "      <td>-2.858345</td>\n",
       "      <td>1.481029</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.217268</td>\n",
       "      <td>4.594756</td>\n",
       "      <td>4.380212</td>\n",
       "      <td>3.271706</td>\n",
       "      <td>-4.442203</td>\n",
       "      <td>-5.418086</td>\n",
       "      <td>0.023346</td>\n",
       "      <td>-2.009643</td>\n",
       "      <td>-0.778441</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.545557</td>\n",
       "      <td>10.026681</td>\n",
       "      <td>2.393387</td>\n",
       "      <td>-2.369646</td>\n",
       "      <td>-0.269603</td>\n",
       "      <td>0.979513</td>\n",
       "      <td>1.661028</td>\n",
       "      <td>0.861730</td>\n",
       "      <td>-1.251917</td>\n",
       "      <td>6.446719</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.476918</td>\n",
       "      <td>-1.510854</td>\n",
       "      <td>-1.485889</td>\n",
       "      <td>2.787948</td>\n",
       "      <td>-1.637217</td>\n",
       "      <td>4.217302</td>\n",
       "      <td>2.820862</td>\n",
       "      <td>3.438934</td>\n",
       "      <td>0.154469</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.593692</td>\n",
       "      <td>3.824784</td>\n",
       "      <td>-0.908609</td>\n",
       "      <td>2.942972</td>\n",
       "      <td>3.894036</td>\n",
       "      <td>3.413266</td>\n",
       "      <td>1.553961</td>\n",
       "      <td>-7.739468</td>\n",
       "      <td>2.197464</td>\n",
       "      <td>0.299972</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.561935</td>\n",
       "      <td>4.208008</td>\n",
       "      <td>7.395545</td>\n",
       "      <td>4.576173</td>\n",
       "      <td>0.815791</td>\n",
       "      <td>-3.038333</td>\n",
       "      <td>-1.339261</td>\n",
       "      <td>7.664257</td>\n",
       "      <td>-2.959643</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.681587</td>\n",
       "      <td>-4.058194</td>\n",
       "      <td>2.212674</td>\n",
       "      <td>-0.258745</td>\n",
       "      <td>-1.462812</td>\n",
       "      <td>-3.909477</td>\n",
       "      <td>3.147509</td>\n",
       "      <td>-4.719408</td>\n",
       "      <td>-6.281705</td>\n",
       "      <td>3.619122</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.608988</td>\n",
       "      <td>0.617678</td>\n",
       "      <td>3.293293</td>\n",
       "      <td>2.675376</td>\n",
       "      <td>-6.577331</td>\n",
       "      <td>-4.715912</td>\n",
       "      <td>-3.124655</td>\n",
       "      <td>-1.824645</td>\n",
       "      <td>-3.132426</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.159427</td>\n",
       "      <td>-4.605655</td>\n",
       "      <td>3.807224</td>\n",
       "      <td>0.908897</td>\n",
       "      <td>10.764171</td>\n",
       "      <td>-3.639272</td>\n",
       "      <td>-3.447677</td>\n",
       "      <td>4.983319</td>\n",
       "      <td>5.159230</td>\n",
       "      <td>1.414902</td>\n",
       "      <td>...</td>\n",
       "      <td>3.107829</td>\n",
       "      <td>4.186121</td>\n",
       "      <td>0.839391</td>\n",
       "      <td>-1.925860</td>\n",
       "      <td>-2.886895</td>\n",
       "      <td>-3.350357</td>\n",
       "      <td>-6.439152</td>\n",
       "      <td>-1.476345</td>\n",
       "      <td>-2.515509</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.633173</td>\n",
       "      <td>3.666814</td>\n",
       "      <td>-0.403462</td>\n",
       "      <td>-3.688655</td>\n",
       "      <td>6.947526</td>\n",
       "      <td>-3.817231</td>\n",
       "      <td>-3.978926</td>\n",
       "      <td>0.316689</td>\n",
       "      <td>5.102090</td>\n",
       "      <td>-3.981173</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.370874</td>\n",
       "      <td>3.227933</td>\n",
       "      <td>-9.640087</td>\n",
       "      <td>5.534013</td>\n",
       "      <td>-3.588567</td>\n",
       "      <td>3.703978</td>\n",
       "      <td>3.262374</td>\n",
       "      <td>3.831146</td>\n",
       "      <td>3.089794</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0          1         2         3          4         5         6   \\\n",
       "0   1.498960   1.740244  2.852685  0.483250   2.629995  2.421617 -0.605894   \n",
       "1  11.043912   1.305697 -4.162067  7.062152  10.188181 -5.216963  0.869844   \n",
       "2   1.699169  -8.643500  5.242382  4.438892  -0.622756  0.935984  5.435414   \n",
       "3   2.454609   4.730429 -1.116898 -5.399067  -1.189977 -1.699942  3.976442   \n",
       "4   0.400635  -2.403447  0.837233  1.377394   4.496939 -3.341007  0.073394   \n",
       "5   2.545557  10.026681  2.393387 -2.369646  -0.269603  0.979513  1.661028   \n",
       "6  -1.593692   3.824784 -0.908609  2.942972   3.894036  3.413266  1.553961   \n",
       "7   3.681587  -4.058194  2.212674 -0.258745  -1.462812 -3.909477  3.147509   \n",
       "8   3.159427  -4.605655  3.807224  0.908897  10.764171 -3.639272 -3.447677   \n",
       "9   1.633173   3.666814 -0.403462 -3.688655   6.947526 -3.817231 -3.978926   \n",
       "\n",
       "         7          8         9   ...         41        42        43  \\\n",
       "0  3.277559  -4.254546  0.657476  ...   1.655511 -5.087583 -1.446621   \n",
       "1  6.395281 -14.499055 -2.626266  ...  -1.371640 -2.518497 -1.723985   \n",
       "2  3.986439   5.502292  2.903077  ...  -2.215406  5.446412  4.758631   \n",
       "3  4.936787   8.205961 -0.676943  ...   1.208745 -1.076917 -1.119831   \n",
       "4 -1.536370  -2.858345  1.481029  ... -11.217268  4.594756  4.380212   \n",
       "5  0.861730  -1.251917  6.446719  ...  -2.476918 -1.510854 -1.485889   \n",
       "6 -7.739468   2.197464  0.299972  ...  -0.561935  4.208008  7.395545   \n",
       "7 -4.719408  -6.281705  3.619122  ...  -7.608988  0.617678  3.293293   \n",
       "8  4.983319   5.159230  1.414902  ...   3.107829  4.186121  0.839391   \n",
       "9  0.316689   5.102090 -3.981173  ... -10.370874  3.227933 -9.640087   \n",
       "\n",
       "         44        45         46        47         48        49   50  \n",
       "0  5.061929 -4.813585  -8.186449  3.429404 -10.006712  3.805926  1.0  \n",
       "1 -2.125003 -2.793473  -0.928793 -7.307003  -9.042577 -1.534964  1.0  \n",
       "2  1.948837 -2.243126 -10.913266  2.709645  -5.339160 -5.383915  1.0  \n",
       "3 -0.596468  3.882105   4.146733  5.821566   5.414165  1.990073  0.0  \n",
       "4  3.271706 -4.442203  -5.418086  0.023346  -2.009643 -0.778441  1.0  \n",
       "5  2.787948 -1.637217   4.217302  2.820862   3.438934  0.154469  0.0  \n",
       "6  4.576173  0.815791  -3.038333 -1.339261   7.664257 -2.959643  0.0  \n",
       "7  2.675376 -6.577331  -4.715912 -3.124655  -1.824645 -3.132426  1.0  \n",
       "8 -1.925860 -2.886895  -3.350357 -6.439152  -1.476345 -2.515509  1.0  \n",
       "9  5.534013 -3.588567   3.703978  3.262374   3.831146  3.089794  0.0  \n",
       "\n",
       "[10 rows x 51 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model...\n",
      "model finished.\n",
      "boundary points started generation...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Maximum allowed dimension exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m optimal_point(df3, model\u001b[38;5;241m=\u001b[39mmodel, desired_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, original_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, resolution\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, undesired_coords\u001b[38;5;241m=\u001b[39mdf3\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m,:df3\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], point_epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.07\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m, in \u001b[0;36moptimal_point\u001b[1;34m(dataset, model, desired_class, original_class, undesired_coords, resolution, point_epsilon, epsilon, constraints, deltas)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboundary points started generation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m boundary_points \u001b[38;5;241m=\u001b[39m compute_decision_boundary_points_all_features(model, X_train, resolution\u001b[38;5;241m=\u001b[39mresolution, epsilon\u001b[38;5;241m=\u001b[39mpoint_epsilon)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboundary points finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Fitting the boundary points to the constraints provided by the real world\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 23\u001b[0m, in \u001b[0;36mcompute_decision_boundary_points_all_features\u001b[1;34m(model, X, resolution, epsilon)\u001b[0m\n\u001b[0;32m     21\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# A grid that contains R^f samples from the f dimensional space where f is the number of features\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m grid \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((resolution \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m n_features, n_features))\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Generates a grid that contains R^f samples based on whether the column contains numeric or categorical values \u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# If the column contains numeric types, then the grid generates a column based on subdividing the numeric columns evenly\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_features):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Checks if the column is not a column consisting of categorical values\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# If it is not categorical, then the column must be numeric. \u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Maximum allowed dimension exceeded"
     ]
    }
   ],
   "source": [
    "optimal_point(df3, model=model, desired_class=0, original_class=1, resolution=15, undesired_coords=df3.iloc[1,:df3.shape[1]-1], point_epsilon=0.1, epsilon=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model...\n",
      "model finished.\n",
      "boundary points started generation...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Maximum allowed dimension exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m optimal_point(df3, model\u001b[38;5;241m=\u001b[39mmodel, desired_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, original_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, resolution\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, undesired_coords\u001b[38;5;241m=\u001b[39mdf3\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m,:df3\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], point_epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.07\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m, in \u001b[0;36moptimal_point\u001b[1;34m(dataset, model, desired_class, original_class, undesired_coords, resolution, point_epsilon, epsilon, constraints, deltas)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboundary points started generation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m boundary_points \u001b[38;5;241m=\u001b[39m compute_decision_boundary_points_all_features(model, X_train, resolution\u001b[38;5;241m=\u001b[39mresolution, epsilon\u001b[38;5;241m=\u001b[39mpoint_epsilon)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboundary points finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Fitting the boundary points to the constraints provided by the real world\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 23\u001b[0m, in \u001b[0;36mcompute_decision_boundary_points_all_features\u001b[1;34m(model, X, resolution, epsilon)\u001b[0m\n\u001b[0;32m     21\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# A grid that contains R^f samples from the f dimensional space where f is the number of features\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m grid \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((resolution \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m n_features, n_features))\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Generates a grid that contains R^f samples based on whether the column contains numeric or categorical values \u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# If the column contains numeric types, then the grid generates a column based on subdividing the numeric columns evenly\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_features):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Checks if the column is not a column consisting of categorical values\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# If it is not categorical, then the column must be numeric. \u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Maximum allowed dimension exceeded"
     ]
    }
   ],
   "source": [
    "optimal_point(df3, model=model, desired_class=0, original_class=1, resolution=10, undesired_coords=df3.iloc[1,:df3.shape[1]-1], point_epsilon=0.1, epsilon=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_binary_search(model, point, opp_point, point_target, opp_target, epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Perform a binary search along the line segment between two points to find the\n",
    "    approximate alpha value where the model's prediction changes from one target\n",
    "    label to another. This is useful for approximating decision boundaries in\n",
    "    binary classification by finding the transition point along a segment connecting\n",
    "    points from opposite classes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        A trained machine learning model with a `predict` method that takes a list\n",
    "        or array of input points and returns predictions as an array. The model\n",
    "        should be a binary classifier (e.g., from scikit-learn, PyTorch, etc.).\n",
    "        Example: sklearn.linear_model.LogisticRegression instance.\n",
    "    \n",
    "    point : numpy.ndarray\n",
    "        A 1D array representing the starting point (feature vector) in the feature\n",
    "        space, typically from one class. Must have the same shape as `opp_point`.\n",
    "    \n",
    "    opp_point : numpy.ndarray\n",
    "        A 1D array representing the opposing point (feature vector) in the feature\n",
    "        space, typically from the opposite class. Must have the same shape as `point`.\n",
    "    \n",
    "    point_target : int or str\n",
    "        The expected prediction label for the `point`. This is used to initialize\n",
    "        the search and compare against the model's prediction at interpolated points.\n",
    "        Should match the model's output format (e.g., 0 or 1 for binary classes).\n",
    "    \n",
    "    opp_target : int or str\n",
    "        The expected prediction label for the `opp_point`. This should be different\n",
    "        from `point_target` and is used to detect when the prediction flips.\n",
    "    \n",
    "    epsilon : float, optional\n",
    "        The tolerance for convergence in the binary search. The loop stops when the\n",
    "        difference between the search bounds is less than this value. Default is 0.01.\n",
    "        Smaller values yield more precise alphas but may increase computation time.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The approximate alpha value (between 0 and 1) where the model's prediction\n",
    "        transitions. A value closer to 0 means the boundary is nearer to `point`,\n",
    "        while closer to 1 means nearer to `opp_point`.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    None explicitly, but may raise exceptions from `model.predict` if the input\n",
    "    shapes are incompatible or if the model is not properly trained.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This function assumes the decision boundary is crossed exactly once along the\n",
    "      line segment; multiple crossings (e.g., in non-linear models) may lead to\n",
    "      approximate or incorrect results.\n",
    "    - The binary search updates bounds based on prediction matches, but if no flip\n",
    "      occurs (e.g., both points predicted the same), it will converge to a midpoint\n",
    "      without a true boundary.\n",
    "    - Usage: Typically called within a loop over pairs of points from different\n",
    "      classes to sample multiple boundary points.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.linear_model import LogisticRegression\n",
    "    >>> model = LogisticRegression().fit(np.array([[0], [1]]), [0, 1])\n",
    "    >>> point = np.array([0.0])\n",
    "    >>> opp_point = np.array([1.0])\n",
    "    >>> alpha = alpha_binary_search(model, point, opp_point, 0, 1, epsilon=0.001)\n",
    "    >>> print(alpha)  # Approximately 0.5 for a linear boundary at 0.5\n",
    "    0.5\n",
    "    \"\"\"\n",
    "    start, end = 0, 1  # Initialize search bounds: 0 at 'point', 1 at 'opp_point'\n",
    "    while abs(end - start) >= epsilon:  # Loop until convergence within epsilon\n",
    "        alpha = (start + end) / 2  # Midpoint alpha (float division ensured)\n",
    "        # Interpolate: weighted average between point and opp_point\n",
    "        temp_candidate = (1 - alpha) * point + alpha * opp_point\n",
    "        # Predict on the interpolated point (assumes model.predict returns array)\n",
    "        temp_target = model.predict([temp_candidate])[0]\n",
    "        if temp_target == point_target: \n",
    "            start = alpha  # Move start bound if prediction matches point's target\n",
    "            point_target = temp_target  # Update target (though often redundant)\n",
    "        elif temp_target == opp_target: \n",
    "            end = alpha  # Move end bound if prediction matches opp's target\n",
    "            opp_target = temp_target  # Update target (though often redundant)\n",
    "    return (start + end) / 2  # Return midpoint as approximate transition alpha\n",
    "\n",
    "\n",
    "def find_decision_boundary(model, X, y, epsilon=1e-3, threshold=10000):\n",
    "    \"\"\"\n",
    "    Approximate the decision boundary of a binary classification model by sampling\n",
    "    points along line segments between correctly classified points from opposite\n",
    "    classes. Uses binary search to find transition points and collects them into\n",
    "    a DataFrame. Handles categorical features by rounding them to integers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        A trained binary classification model with a `predict` method that takes\n",
    "        a list or array of input points and returns predictions as an array.\n",
    "        Example: sklearn.svm.SVC instance.\n",
    "    \n",
    "    X : pandas.DataFrame\n",
    "        The feature dataset, where rows are samples and columns are features.\n",
    "        Supports mixed types, including integer categoricals.\n",
    "    \n",
    "    y : pandas.Series or numpy.ndarray\n",
    "        The target labels corresponding to X. Must contain exactly two unique\n",
    "        binary labels (e.g., 0 and 1).\n",
    "    \n",
    "    epsilon : float, optional\n",
    "        The precision for the binary search in `alpha_binary_search`. Smaller\n",
    "        values increase accuracy but computation time. Default is 1e-3.\n",
    "    \n",
    "    threshold : int, optional\n",
    "        The maximum number of boundary points to generate. Stops early if reached\n",
    "        to prevent excessive computation on large datasets. Default is 10000.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame containing the approximated boundary points, with the same\n",
    "        columns as X. Categorical columns (detected as int types) are converted\n",
    "        to integers.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If y does not contain exactly two unique labels (non-binary classification).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This function clusters points by true labels (y), then filters pairs where\n",
    "      the model correctly predicts them (to ensure opposite sides of the boundary).\n",
    "    - It may miss boundaries if the model has high error rates (few correct pairs).\n",
    "    - Computational complexity is O(n*m) where n and m are cluster sizes, capped\n",
    "      by threshold. For large datasets, reduce threshold or sample clusters.\n",
    "    - A `bool_vec` is created but unused; it may be a remnant for future masking\n",
    "      (e.g., to ignore categoricals in interpolation).\n",
    "    - Categorical features are auto-detected as int columns and rounded to int\n",
    "      in the output for interpretability.\n",
    "    - Usage: Call after training a model to visualize or analyze its boundary,\n",
    "      e.g., plot the points in 2D or use for explanations.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> import pandas as pd\n",
    "    >>> from sklearn.svm import SVC\n",
    "    >>> X = pd.DataFrame({'feat1': [0, 1, 2], 'feat2': [0, 1, 0]})\n",
    "    >>> y = np.array([0, 1, 0])\n",
    "    >>> model = SVC(kernel='linear').fit(X, y)\n",
    "    >>> boundary = find_decision_boundary(model, X, y, epsilon=0.001, threshold=5)\n",
    "    >>> print(boundary.shape)  # e.g., (number_of_points, 2)\n",
    "    (2, 2)\n",
    "    \"\"\"\n",
    "    # Detect categorical features (assumed as int columns)\n",
    "    categorical_features = X.select_dtypes(include=int).columns.tolist()\n",
    "    cat_indices = [X.columns.get_loc(col) for col in categorical_features]\n",
    "\n",
    "    # Create a boolean vector (1 for continuous, 0 for categorical; currently unused)\n",
    "    bool_vec = [1] * (len(X.columns)) \n",
    "    for i in range(len(cat_indices)): \n",
    "        bool_vec[cat_indices[i]] = 0 \n",
    "\n",
    "    X_np = X.to_numpy()  # Convert features to NumPy for efficient ops\n",
    "    y_np = y.to_numpy() if not isinstance(y, np.ndarray) else y  # Ensure y is NumPy\n",
    "    boundary_points = []  # List to collect boundary point arrays\n",
    "    unique_labels = np.unique(y_np)  # Get unique class labels\n",
    "    if len(unique_labels) != 2:\n",
    "        raise ValueError(\"Only supports binary classification.\")\n",
    "    \n",
    "    label_a, label_b = unique_labels[0], unique_labels[1]  # Assign labels\n",
    "\n",
    "    # Cluster points by true labels\n",
    "    cluster_a = X_np[y_np == label_a]\n",
    "    cluster_b = X_np[y_np == label_b]\n",
    "\n",
    "    total_N = 0  # Counter for generated points\n",
    "    for i in range(cluster_a.shape[0]):\n",
    "        point = cluster_a[i]\n",
    "        pt_pred = model.predict([point])  # Predict on point from cluster A\n",
    "\n",
    "        for j in range(cluster_b.shape[0]): \n",
    "            match_point = cluster_b[j]\n",
    "            match_pt_pred = model.predict([match_point])  # Predict on point from B\n",
    "            # Check if model correctly classifies both (ensures opposite sides)\n",
    "            if pt_pred.item() == label_a and match_pt_pred.item() == label_b: \n",
    "                # Find alpha where prediction flips\n",
    "                alpha = alpha_binary_search(model, point, match_point, label_a, label_b, epsilon=epsilon)\n",
    "                # Compute boundary point via interpolation\n",
    "                boundary = (1 - alpha) * point + alpha * match_point\n",
    "                boundary_points.append(boundary)\n",
    "\n",
    "                total_N += 1\n",
    "                if total_N >= threshold:  # Early stop inner loop\n",
    "                    break\n",
    "        if total_N >= threshold:  # Early stop outer loop\n",
    "            break\n",
    "    \n",
    "    # Convert list to DataFrame with original columns\n",
    "    boundary_pts = pd.DataFrame(data=boundary_points, columns=X.columns)\n",
    "\n",
    "    # Round categoricals to int for discrete values\n",
    "    for col in categorical_features: \n",
    "        boundary_pts[col] = boundary_pts[col].astype(int)\n",
    "\n",
    "    return boundary_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_point(dataset, model, desired_class, original_class, undesired_coords, threshold=10000, point_epsilon=0.1, epsilon=0.01, constraints=[], deltas=[]): \n",
    "    \"\"\"\n",
    "    Finds the closest point to the decision boundary from an undesired point,\n",
    "    optionally constrained by real-world conditions.\n",
    "    This essentially finds the counterfactual explanation for a given point by minimizing the distance to the given boundary.\n",
    "    This method is important because it addresses a key problem with the original optimal_point() function where we generated an R^n dimensional grid that we would then have to iterate over. \n",
    "    The problem with iterating over such a grid is eventually that we will hit a memory error for high-dimensional features such as 20, 30 or 40 features. This will cause the function to crash. \n",
    "    Additionally, due to the exponential increase of the number of features to search, the grid will become infeasible to search (curse of dimensionality). \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pd.DataFrame\n",
    "        Full dataset containing features and a final column with class labels.\n",
    "    \n",
    "    model : sklearn-like classifier\n",
    "        A binary classification model with a `.fit()` and `.predict()` method.\n",
    "    \n",
    "    desired_class : int or label\n",
    "        The target class we want the corrected point to belong to.\n",
    "    \n",
    "    original_class : int or label\n",
    "        The actual class label of the undesired point.\n",
    "    \n",
    "    undesired_coords : list or array\n",
    "        The coordinates of the original (\"unhealthy\") point.\n",
    "    \n",
    "    threshold : int, optional\n",
    "        Max number of decision boundary points to sample. Default is 10000.\n",
    "    \n",
    "    point_epsilon : float, optional\n",
    "        Precision used to estimate decision boundary points. Default is 0.1.\n",
    "    \n",
    "    epsilon : float, optional\n",
    "        Step size used when displacing a point toward the decision boundary. Default is 0.01.\n",
    "    \n",
    "    constraints : list, optional\n",
    "        A list of real-world constraints on the features (e.g., ranges, logic constraints). Default is [].\n",
    "    \n",
    "    deltas : list, optional\n",
    "        Tolerances or maximum displacements for each feature. Default is [].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        A corrected point that satisfies the class change and real-world constraints.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    Exception\n",
    "        If the number of constraints exceeds the number of features.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This function trains the model on the provided dataset, generates boundary points using\n",
    "      `find_decision_boundary`, applies constraints, and finds the closest optimal point.\n",
    "    - Assumes binary classification and relies on external functions like `real_world_constraints`,\n",
    "      `closest_point`, `move_from_A_to_B_with_x1_displacement`, etc., which must be defined elsewhere.\n",
    "    - Includes plotting for visualization (e.g., boundary contours, points), which requires matplotlib.\n",
    "    - The function blends boundary approximation with counterfactual generation, useful for explainable AI.\n",
    "    - Print statements are for progress tracking; plotting is partially commented out but can be enabled.\n",
    "    - Usage: Call with a dataset and model to generate counterfactuals, e.g., for model interpretation or optimization.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> from sklearn.linear_model import LogisticRegression\n",
    "    >>> dataset = pd.DataFrame({'feat1': [0, 1, 2], 'feat2': [0, 1, 0], 'label': [0, 1, 0]})\n",
    "    >>> model = LogisticRegression()\n",
    "    >>> undesired_coords = [2, 0]  # Example point from class 0\n",
    "    >>> optimal = optimal_point(dataset, model, desired_class=1, original_class=0, undesired_coords=undesired_coords)\n",
    "    >>> print(optimal)  # e.g., array([[1.5, 0.5]])\n",
    "    \"\"\"\n",
    "    # -------------------------------\n",
    "    # STEP 1: Train the model\n",
    "    # -------------------------------\n",
    "    X_train = dataset.iloc[:, 0:-1]  # Extract features from dataset\n",
    "    y_train = dataset.iloc[:, -1]  # Extract labels from dataset\n",
    "    n_features = X_train.shape[1]  # Get number of features\n",
    "\n",
    "    print(\"fitting model...\")\n",
    "    model.fit(X_train, y_train)  # Train the model on the dataset\n",
    "    print(\"model finished.\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # STEP 2: Find decision boundary\n",
    "    # -------------------------------\n",
    "    print(\"boundary points started generation...\")\n",
    "\n",
    "    # This step uses binary interpolation to get points close to the decision boundary\n",
    "    boundary_points = find_decision_boundary(model, X_train, y_train,\n",
    "                                             threshold=threshold, epsilon=point_epsilon)\n",
    "    print(\"boundary points finished.\")\n",
    "    print(boundary_points.shape)\n",
    "\n",
    "    # -------------------------------\n",
    "    # STEP 3: Apply real-world constraints (optional)\n",
    "    # -------------------------------\n",
    "    # Reduce boundary points based on external rules (e.g., cost limits, physics constraints)\n",
    "    contours = real_world_constraints(points=boundary_points,\n",
    "                                      undesired_coords=undesired_coords,\n",
    "                                      constraints=constraints)\n",
    "    contours = np.unique(contours.to_numpy(), axis=0)  # Remove duplicates from constrained points\n",
    "    undesired_datapt = np.reshape(np.array(list(undesired_coords)), (1, -1))  # Reshape undesired point to 2D array\n",
    "\n",
    "    # -------------------------------\n",
    "    # STEP 4: Find closest point on constrained boundary\n",
    "    # -------------------------------\n",
    "    print(\"Finding the closest point from the contour line to the point...\")\n",
    "    optimal_datapt = closest_point(undesired_datapt, contour=contours)\n",
    "    print(\"Finding the closest point from the contour line to the point.\")  # Note: Duplicate print, possibly a typo\n",
    "    #plt.plot(contours[:,0], contours[:,1], lw=0.5, color='red')  # Commented: Plot contours for visualization\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # STEP 5: Post-process based on class flip requirement\n",
    "    # -------------------------------\n",
    "\n",
    "    # If we want to *flip* the class of the point...\n",
    "    if desired_class != original_class: \n",
    "         # Move in the direction of the boundary, slightly overshooting\n",
    "        D = optimal_datapt - undesired_datapt  # Compute direction vector\n",
    "        deltas = D * (1+epsilon)  # Scale by (1 + epsilon) to overshoot\n",
    "        optimal_datapt = move_from_A_to_B_with_x1_displacement(undesired_datapt, optimal_datapt, deltas=deltas)\n",
    "    else:\n",
    "        # If we want to *stay within* the same class (more constrained)\n",
    "        closest_boundedpt = None\n",
    "        deltas, len_constr = det_constraints(datapt=undesired_datapt[0], deltas=deltas)  # Determine constraints\n",
    "\n",
    "        if len_constr > X_train.shape[1]:\n",
    "            raise Exception(\"There cannot be more constraints than features\")\n",
    "\n",
    "        elif len_constr == X_train.shape[1]:\n",
    "            # All n dimensions are constrained, so generate an exact grid of boundary candidates\n",
    "            bounded_contour_pts = get_multi_dim_border_points(center=undesired_datapt[0],\n",
    "                                                              extents=deltas,\n",
    "                                                              step=0.05)\n",
    "            np_bounded_contour = np.array(bounded_contour_pts)  # Convert to NumPy array\n",
    "            x_values, y_values = np_bounded_contour[:, 0], np_bounded_contour[:, 1]  # Extract x/y for plotting\n",
    "            plt.scatter(x_values, y_values, color='blue', marker='o')  # Plot bounded points\n",
    "            closest_boundedpt = closest_border_point(bounded_contour_pts, contour=contours)  # Find closest on border\n",
    "\n",
    "        else:\n",
    "            # Partially constrained - less than n dimensions are constrained\n",
    "            bounded_contour_pts = constraint_bounds(contours, undesired_datapt, deltas)  # Apply partial bounds\n",
    "            closest_boundedpt = closest_point(point=undesired_datapt, contour=bounded_contour_pts)  # Find closest\n",
    "        \n",
    "        D = closest_boundedpt - undesired_datapt  # Compute direction\n",
    "        optimal_datapt = move_from_A_to_B_with_x1_displacement(undesired_datapt, closest_boundedpt, deltas=D)  # Move point\n",
    "    \n",
    "    # Plot original and optimal points with connecting line\n",
    "    plt.scatter(undesired_datapt[0][0], undesired_datapt[0][1], c = 'r')  # Plot undesired point\n",
    "    plt.text(undesired_datapt[0][0]+0.002, undesired_datapt[0][1]+0.002, 'NH')  # Label 'NH' (e.g., Non-Healthy)\n",
    "    plt.scatter(optimal_datapt[0][0], optimal_datapt[0][1], c = 'g')  # Plot optimal point (changed to green for distinction)\n",
    "    plt.text(optimal_datapt[0][0]+0.002, optimal_datapt[0][1]+0.002, 'H')  # Label 'H' (e.g., Healthy; adjusted from duplicate 'NH')\n",
    "    plt.plot([undesired_datapt[0][0], optimal_datapt[0][0]], [undesired_datapt[0][1],optimal_datapt[0][1]], linestyle='--')  # Dashed line between points\n",
    "    return optimal_datapt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments of the Computation Costs for Gridless Approach (Logistic Regression)\n",
    "\n",
    "$\\textbf{ Logistic Regression with Gridless Method }$ -- 50 features\n",
    "\n",
    "Search: $S = 1,000,000$, 29.7 second runtime, $50,000$ boundary points found\n",
    "\n",
    "Search: $S = 200,000$, 6.1 second runtime, $10,000$ boundary points found\n",
    "\n",
    "$\\textbf{ Logistic Regression with Gridless Method }$ -- 10 features\n",
    "\n",
    "Search: $S = 2,000,000$, 58.6 second runtime, $100,000$ boundary points found\n",
    "\n",
    "Search: $S = 200,000$, 5.9 second runtime, $10,000$ boundary points found\n",
    "\n",
    "\n",
    "$\\textbf{ Logistic Regression with Gridless Method }$ -- 2 features\n",
    "\n",
    "Search: $S = 30,000$, 0.9 seconds runtime, $1,500$ boundary points found\n",
    "\n",
    "Search: $S = 10,000$, 0.3 seconds runtime, $500$ boundary points found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model...\n",
      "model finished.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(1500, 2)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Finding the closest point from the contour line to the point.\n",
      "[[1.07091351 1.09432123]]\n",
      "[[-1.1723115  -0.04499449]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.06834554,  1.11054874]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_point(df1, model=model, desired_class=0, original_class=1, threshold=1500, undesired_coords=df1.iloc[1,:df1.shape[1]-1], point_epsilon=1e-6, epsilon=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model...\n",
      "model finished.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(500, 2)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Finding the closest point from the contour line to the point.\n",
      "[[1.07091407 1.08740554]]\n",
      "[[-1.17158501 -0.06247468]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.06756819,  1.09185185]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_point(df1, model=model, desired_class=0, original_class=1, threshold=500, undesired_coords=df1.iloc[1,:df1.shape[1]-1], point_epsilon=1e-6, epsilon=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model...\n",
      "model finished.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(10000, 10)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Finding the closest point from the contour line to the point.\n",
      "[[1.08086181 1.08009778 1.05649038 1.34857541 1.06644951 1.08088977\n",
      "  1.0904509  1.07676375 1.07914465 1.07358859]]\n",
      "[[-0.09951026 -0.10696393  0.07820282 -0.00484097  0.30036689 -0.09925739\n",
      "  -0.05332044 -0.15919617 -0.11800827 -0.29916742]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.23479959,  0.82742501,  3.14040285,  0.0531383 , -0.29099069,\n",
       "        -1.09280704,  0.26570737,  0.68230259,  1.61368784,  1.74813994]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_point(df2, model=model, desired_class=0, original_class=1, threshold=10000, undesired_coords=df2.iloc[1,:df2.shape[1]-1], point_epsilon=1e-6, epsilon=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model...\n",
      "model finished.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(100000, 10)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Finding the closest point from the contour line to the point.\n",
      "[[1.04479475 1.07618195 1.10204599 1.07415725 1.06247779 1.0765085\n",
      "  1.37979831 1.07430465 1.06180044 1.07748248]]\n",
      "[[ 0.04145148 -0.17408467 -0.03438951 -0.25838176  0.14124536 -0.16540036\n",
      "  -0.00445386 -0.24956845  0.12949476 -0.14400074]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.38566472,  0.75560974,  3.0198835 , -0.21787592, -0.46124675,\n",
       "        -1.16357564,  0.31770526,  0.5856067 ,  1.87853343,  1.91416439]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_point(df2, model=model, desired_class=0, original_class=1, threshold=100000, undesired_coords=df2.iloc[1,:df2.shape[1]-1], point_epsilon=1e-6, epsilon=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model...\n",
      "model finished.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(10000, 50)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Finding the closest point from the contour line to the point.\n",
      "[[1.07023637 1.06976976 1.06969373 1.07021297 1.07019208 1.06967599\n",
      "  1.06746763 1.07036306 1.06984842 1.06977869 1.07058837 1.0702381\n",
      "  1.06975027 1.06972009 1.06973857 1.0712423  1.07144123 1.07081446\n",
      "  1.06865035 1.07067915 1.06942924 1.07035981 1.06899849 1.07111809\n",
      "  1.06024232 1.07163432 1.06724779 1.06810594 1.0702466  1.06864233\n",
      "  1.07050402 1.07028733 1.07047349 1.07057354 1.06968933 1.07808458\n",
      "  1.07140577 1.06982413 1.07138016 1.07078645 1.07023026 1.07182012\n",
      "  1.06801061 1.06163103 1.06959137 1.06826589 1.06960988 1.06980174\n",
      "  1.06983912 1.06881256]]\n",
      "[[-4.52777119  4.64638377  3.49262824 -5.02509248 -5.57172006  3.30137171\n",
      "   0.42152892 -2.94817682  7.05783795  4.83392349 -1.81956897 -4.49499512\n",
      "   4.28371073  3.82166172  4.09181003 -0.86230475 -0.74342181 -1.31475663\n",
      "   0.79180003 -1.57650486  1.87370297 -2.97480682  1.06739194 -0.95799257\n",
      "   0.10865719 -0.65570784  0.38777898  0.56392275 -4.34009765  0.78711262\n",
      "  -2.12391122 -3.72495079 -2.26083433 -1.86661375  3.44317249 -0.13335077\n",
      "  -0.76214682  6.08308136 -0.77627239 -1.36154189 -4.64791845 -0.58887274\n",
      "   0.53685444  0.1268532   2.61753238  0.61603255  2.74174705  5.3959449\n",
      "   6.64992582  0.90009954]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 6.19812621,  6.27625802, -0.42602433,  1.68423239,  4.22537044,\n",
       "        -1.6855653 ,  1.31981248,  3.23966109, -6.94823857,  2.54496262,\n",
       "        -0.56307918, -2.99339257,  2.10512098,  3.38857828,  4.87270043,\n",
       "        -0.82116314,  0.38646645,  1.00855226, -1.63925939, -3.04571639,\n",
       "        -2.1559183 ,  5.84787037, -0.72097048, -0.38259003, -1.73338932,\n",
       "         1.02140087,  3.84590092,  2.38359545,  2.76937251, -5.68655905,\n",
       "         1.02765196, -0.44046882, -6.08452679, -1.51411165,  3.39464693,\n",
       "         2.52157268,  4.76625411,  2.07175185,  4.04757458, -0.70950234,\n",
       "         1.51880869, -2.00280518, -1.94513123, -1.58931375,  0.67468693,\n",
       "        -2.13538688,  2.00380672, -1.534412  , -1.92822669, -0.57292597]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_point(df3, model=model, desired_class=0, original_class=1, threshold=10000, undesired_coords=df3.iloc[1,:df3.shape[1]-1], point_epsilon=1e-6, epsilon=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model...\n",
      "model finished.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(50000, 50)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Finding the closest point from the contour line to the point.\n",
      "[[1.07023637 1.06976976 1.06969373 1.07021297 1.07019208 1.06967599\n",
      "  1.06746763 1.07036306 1.06984842 1.06977869 1.07058837 1.0702381\n",
      "  1.06975027 1.06972009 1.06973857 1.0712423  1.07144123 1.07081446\n",
      "  1.06865035 1.07067915 1.06942924 1.07035981 1.06899849 1.07111809\n",
      "  1.06024232 1.07163432 1.06724779 1.06810594 1.0702466  1.06864233\n",
      "  1.07050402 1.07028733 1.07047349 1.07057354 1.06968933 1.07808458\n",
      "  1.07140577 1.06982413 1.07138016 1.07078645 1.07023026 1.07182012\n",
      "  1.06801061 1.06163103 1.06959137 1.06826589 1.06960988 1.06980174\n",
      "  1.06983912 1.06881256]]\n",
      "[[-4.52777119  4.64638377  3.49262824 -5.02509248 -5.57172006  3.30137171\n",
      "   0.42152892 -2.94817682  7.05783795  4.83392349 -1.81956897 -4.49499512\n",
      "   4.28371073  3.82166172  4.09181003 -0.86230475 -0.74342181 -1.31475663\n",
      "   0.79180003 -1.57650486  1.87370297 -2.97480682  1.06739194 -0.95799257\n",
      "   0.10865719 -0.65570784  0.38777898  0.56392275 -4.34009765  0.78711262\n",
      "  -2.12391122 -3.72495079 -2.26083433 -1.86661375  3.44317249 -0.13335077\n",
      "  -0.76214682  6.08308136 -0.77627239 -1.36154189 -4.64791845 -0.58887274\n",
      "   0.53685444  0.1268532   2.61753238  0.61603255  2.74174705  5.3959449\n",
      "   6.64992582  0.90009954]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 6.19812621,  6.27625802, -0.42602433,  1.68423239,  4.22537044,\n",
       "        -1.6855653 ,  1.31981248,  3.23966109, -6.94823857,  2.54496262,\n",
       "        -0.56307918, -2.99339257,  2.10512098,  3.38857828,  4.87270043,\n",
       "        -0.82116314,  0.38646645,  1.00855226, -1.63925939, -3.04571639,\n",
       "        -2.1559183 ,  5.84787037, -0.72097048, -0.38259003, -1.73338932,\n",
       "         1.02140087,  3.84590092,  2.38359545,  2.76937251, -5.68655905,\n",
       "         1.02765196, -0.44046882, -6.08452679, -1.51411165,  3.39464693,\n",
       "         2.52157268,  4.76625411,  2.07175185,  4.04757458, -0.70950234,\n",
       "         1.51880869, -2.00280518, -1.94513123, -1.58931375,  0.67468693,\n",
       "        -2.13538688,  2.00380672, -1.534412  , -1.92822669, -0.57292597]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_point(df3, model=model, desired_class=0, original_class=1, threshold=50000, undesired_coords=df3.iloc[1,:df3.shape[1]-1], point_epsilon=1e-6, epsilon=0.07)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
