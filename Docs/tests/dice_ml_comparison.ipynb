{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0b26986",
   "metadata": {},
   "source": [
    "# DiCE ML Comparison with Optimal Point Method \n",
    "\n",
    "We compare the DiCE Model-agnostic methods with the optimal point method in this notebook. \n",
    "\n",
    "1. First, we import DiCE ML model-agnostic methods \n",
    "2. Second, we import the packaged files needed to run the \"Optimal Point\" Methodology \n",
    "3. Third, we run the experiments and compare the results at the end using different models such as SVM and random forest classifier.\n",
    "4. Finally, we compare the runtimes of DiCE and the Optimal Point methodology\n",
    "\n",
    "Note: Running experiments for the adult income can take hours. Please be mindful of the runtime for these experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beef340",
   "metadata": {},
   "source": [
    "# Step 1: Importing DiCE ML and helper functions \n",
    "\n",
    "Below we import DiCE ML and their relevant helper functions. We import the sci-kit learn library and some of their necessary methods to make sure that we can run the experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import DiCE\n",
    "import dice_ml\n",
    "from dice_ml.utils import helpers  # helper functions\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplot\n",
    "\n",
    "import json \n",
    "import datetime  \n",
    "\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7874df33",
   "metadata": {},
   "source": [
    "# Step 2: Import the necessary functionality to make Optimal Point methodology work \n",
    "\n",
    "We import many of the methods needed for the ```optimal_point()``` function to work as intended below. We import additional methods from ```binary_search_optimal_point().```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a8f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from files.common_functions import euclidean_distance, closest_border_point, closest_point, move_from_A_to_B_with_x1_displacement\n",
    "from files.common_functions import get_multi_dim_border_points, det_constraints, real_world_constraints, constraint_bounds\n",
    "from files.common_functions import balance_dataset, check_class_balance, convert_columns\n",
    "from files.binary_search_optimal_point import find_decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54229082",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../../toy_dataset.csv')\n",
    "# SVM classifier with polynomial decision boundary\n",
    "svm_classifier = svm.SVC(kernel='poly',C=10, degree=2, probability=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77647d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dataset():\n",
    "    X1=df['x1']\n",
    "    X2=df['x2']\n",
    "    X_train=np.array(list(zip(X1,X2)))\n",
    "    y_train=df['y'].values\n",
    "    # svm_classifier = svm.SVC(kernel='linear', C=10)\n",
    "    #svm_classifier = svm.SVC(kernel='poly', C=10)\n",
    "    svm_classifier = svm.SVC(kernel='poly',C=10, degree=2)\n",
    "    # svm_classifier = svm.SVC(kernel='rbf', gamma=0.1, C=100)\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    color_ls = []\n",
    "    for k in y_train:\n",
    "        if k == 1:\n",
    "            color_ls.append('b')\n",
    "        else:\n",
    "            color_ls.append('r')\n",
    "    color_ls\n",
    "    label = []\n",
    "    for k in y_train:\n",
    "        if k == 1:\n",
    "            label.append('H')\n",
    "        else:\n",
    "            label.append('NH')\n",
    "    label\n",
    "    def plot_decision_boundary(clf, X, y):\n",
    "        x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max),np.arange(x2_min, x2_max))\n",
    "        Z = clf.decision_function(np.array([xx1.ravel(), xx2.ravel()]).T).reshape(xx1.shape)\n",
    "        m = np.nonzero(Z < -19)\n",
    "        Q = clf.predict(np.array([xx1.ravel(), xx2.ravel()]).T).reshape(xx1.shape)\n",
    "        plt.contourf(xx1, xx2, Z, colors='k', levels=[-1, 0, 1], alpha=0.4,linestyles=['--', '-', '--'])\n",
    "        plt.xlim(xx1.min(), xx1.max())\n",
    "        plt.ylim(xx2.min(), xx2.max())\n",
    "        \n",
    "    plot_decision_boundary(svm_classifier, X_train, y_train)\n",
    "\n",
    "    for k, (i,j) in enumerate(X_train):\n",
    "        plt.scatter(i, j, c = color_ls[k])\n",
    "        plt.text(i+0.02, j+0.02, label[k])\n",
    "\n",
    "visualize_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf44886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_point(dataset, model, desired_class, original_class, chosen_row=-1, threshold=10000, point_epsilon=0.1, epsilon=0.01, constraints=[], deltas=[], plot=False): \n",
    "    \"\"\"\n",
    "    Finds the closest point to the decision boundary from an undesired point,\n",
    "    optionally constrained by real-world conditions.\n",
    "    This essentially finds the counterfactual explanation for a given point by minimizing the distance to the given boundary.\n",
    "    This method is important because it addresses a key problem with the original optimal_point() function where we generated an R^n dimensional grid that we would then have to iterate over. \n",
    "    The problem with iterating over such a grid is eventually that we will hit a memory error for high-dimensional features such as 20, 30 or 40 features. This will cause the function to crash. \n",
    "    Additionally, due to the exponential increase of the number of features to search, the grid will become infeasible to search (curse of dimensionality). \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pd.DataFrame\n",
    "        Full dataset containing features and a final column with class labels.\n",
    "    \n",
    "    model : sklearn-like classifier\n",
    "        A binary classification model with a `.fit()` and `.predict()` method.\n",
    "    \n",
    "    desired_class : int or label\n",
    "        The target class we want the corrected point to belong to.\n",
    "    \n",
    "    original_class : int or label\n",
    "        The actual class label of the undesired point.\n",
    "    \n",
    "    chosen_row :  int \n",
    "        The selected row of the dataset to find the counterfactual explanation for\n",
    "    \n",
    "    threshold : int, optional\n",
    "        Max number of decision boundary points to sample. Default is 10000.\n",
    "    \n",
    "    point_epsilon : float, optional\n",
    "        Precision used to estimate decision boundary points. Default is 0.1.\n",
    "    \n",
    "    epsilon : float, optional\n",
    "        Step size used when displacing a point toward the decision boundary. Default is 0.01.\n",
    "    \n",
    "    constraints : list, optional\n",
    "        A list of real-world constraints on the features (e.g., ranges, logic constraints). Default is [].\n",
    "    \n",
    "    deltas : list, optional\n",
    "        Tolerances or maximum displacements for each feature. Default is [].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        A corrected point that satisfies the class change and real-world constraints.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    Exception\n",
    "        If the number of constraints exceeds the number of features.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This function trains the model on the provided dataset, generates boundary points using\n",
    "      `find_decision_boundary`, applies constraints, and finds the closest optimal point.\n",
    "    - Assumes binary classification and relies on external functions like `real_world_constraints`,\n",
    "      `closest_point`, `move_from_A_to_B_with_x1_displacement`, etc., which must be defined elsewhere.\n",
    "    - Includes plotting for visualization (e.g., boundary contours, points), which requires matplotlib.\n",
    "    - The function blends boundary approximation with counterfactual generation, useful for explainable AI.\n",
    "    - Print statements are for progress tracking; plotting is partially commented out but can be enabled.\n",
    "    - Usage: Call with a dataset and model to generate counterfactuals, e.g., for model interpretation or optimization.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> from sklearn.linear_model import LogisticRegression\n",
    "    >>> dataset = pd.DataFrame({'feat1': [0, 1, 2], 'feat2': [0, 1, 0], 'label': [0, 1, 0]})\n",
    "    >>> model = LogisticRegression()\n",
    "    >>> undesired_coords = [2, 0]  # Example point from class 0\n",
    "    >>> optimal = optimal_point(dataset, model, desired_class=1, original_class=0, undesired_coords=undesired_coords)\n",
    "    >>> print(optimal)  # e.g., array([[1.5, 0.5]])\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert categorical columns if needed (before balancing)\n",
    "    inv_col_map = convert_columns(dataset)\n",
    "\n",
    "    # Extract features and labels before balancing\n",
    "    X_orig = dataset.iloc[:, :-1]\n",
    "    \n",
    "    # Save the original row's feature values\n",
    "    undesired_coords = X_orig.iloc[chosen_row, :].copy()\n",
    "\n",
    "    # Balance the dataset\n",
    "    dataset = balance_dataset(df=dataset, target=dataset.columns[-1])\n",
    "    \n",
    "    if not check_class_balance(dataset, target=dataset.columns[-1]):\n",
    "        raise RuntimeError(\"Failed to balance classes for binary classification\")\n",
    "    \n",
    "    sampled_dataset = dataset.sample(n=min(dataset.shape[0], 10000))\n",
    "\n",
    "    # Extract new training features/labels after balancing\n",
    "    X_train = sampled_dataset.iloc[:, :-1]\n",
    "    y_train = sampled_dataset.iloc[:, -1]\n",
    "    # Train the model\n",
    "    print(\"Fitting model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # STEP 2: Find decision boundary\n",
    "    # -------------------------------\n",
    "    print(\"boundary points started generation...\")\n",
    "\n",
    "    # This step uses binary interpolation to get points close to the decision boundary\n",
    "    boundary_points = find_decision_boundary(model, X_train, y_train,\n",
    "                                             threshold=threshold, epsilon=point_epsilon)\n",
    "    print(\"boundary points finished.\")\n",
    "    print(boundary_points.shape)\n",
    "\n",
    "    # -------------------------------\n",
    "    # STEP 3: Apply real-world constraints (optional)\n",
    "    # -------------------------------\n",
    "    # Reduce boundary points based on external rules (e.g., cost limits, physics constraints)\n",
    "    contours = real_world_constraints(points=boundary_points,\n",
    "                                      undesired_coords=undesired_coords,\n",
    "                                      constraints=constraints)\n",
    "    contours_pd = np.unique(contours.to_numpy(), axis=0)  # Remove duplicates from constrained points\n",
    "    undesired_datapt = np.reshape(undesired_coords, (1, -1))  # Reshape undesired point to 2D array\n",
    "\n",
    "    # -------------------------------\n",
    "    # STEP 4: Find closest point on constrained boundary\n",
    "    # -------------------------------\n",
    "    print(\"Finding the closest point from the contour line to the point...\")\n",
    "    optimal_datapt = closest_point(undesired_datapt, contour=contours)\n",
    "    print(\"Finding the closest point from the contour line to the point.\")  # Note: Duplicate print, possibly a typo\n",
    "    if plot:\n",
    "        plt.plot(contours[:,0], contours[:,1], lw=0.5, color='red')  # Commented: Plot contours for visualization\n",
    "\n",
    "    # -------------------------------\n",
    "    # STEP 5: Post-process based on class flip requirement\n",
    "    # -------------------------------\n",
    "\n",
    "    # If we want to *flip* the class of the point...\n",
    "    if desired_class != original_class: \n",
    "         # Move in the direction of the boundary, slightly overshooting\n",
    "        D = optimal_datapt - undesired_datapt  # Compute direction vector\n",
    "        deltas = D * (1+epsilon)  # Scale by (1 + epsilon) to overshoot\n",
    "        optimal_datapt = move_from_A_to_B_with_x1_displacement(undesired_datapt, optimal_datapt, deltas=deltas)\n",
    "    else:\n",
    "        # If we want to *stay within* the same class (more constrained)\n",
    "        closest_boundedpt = None\n",
    "        deltas, len_constr = det_constraints(datapt=undesired_datapt[0], deltas=deltas)  # Determine constraints\n",
    "\n",
    "        if len_constr > X_train.shape[1]:\n",
    "            raise Exception(\"There cannot be more constraints than features\")\n",
    "\n",
    "        elif len_constr == X_train.shape[1]:\n",
    "            # All n dimensions are constrained, so generate an exact grid of boundary candidates\n",
    "            bounded_contour_pts = get_multi_dim_border_points(center=undesired_datapt[0],\n",
    "                                                              extents=deltas,\n",
    "                                                              step=0.05)\n",
    "            np_bounded_contour = np.array(bounded_contour_pts)  # Convert to NumPy array\n",
    "            x_values, y_values = np_bounded_contour[:, 0], np_bounded_contour[:, 1]  # Extract x/y for plotting\n",
    "            #plt.scatter(x_values, y_values, marker='o')  # Plot bounded points\n",
    "            closest_boundedpt = closest_border_point(bounded_contour_pts, contour=contours)  # Find closest on border\n",
    "\n",
    "        else:\n",
    "            # Partially constrained - less than n dimensions are constrained\n",
    "            bounded_contour_pts = constraint_bounds(contours, undesired_datapt, deltas)  # Apply partial bounds\n",
    "            closest_boundedpt = closest_point(point=undesired_datapt, contour=bounded_contour_pts)  # Find closest\n",
    "        \n",
    "        D = closest_boundedpt - undesired_datapt  # Compute direction\n",
    "        optimal_datapt = move_from_A_to_B_with_x1_displacement(undesired_datapt, closest_boundedpt, deltas=D)  # Move point\n",
    "    \n",
    "    # Plot original and optimal points with connecting line\n",
    "    if plot and contours_pd is not None:\n",
    "        contours = contours_pd.to_numpy()\n",
    "        params = {'mathtext.default': 'regular' }\n",
    "        plt.rcParams.update(params)\n",
    "        plt.scatter(contours[:,0], contours[:,1], lw=0.5, color='purple', label=\"Decision Boundary Points\")  # Commented: Plot contours for visualization\n",
    "        plt.scatter(undesired_datapt[0][0], undesired_datapt[0][1], c = 'r', label=\"NH: Not Healthy\")  # Plot undesired point\n",
    "        plt.text(undesired_datapt[0][0]+0.002, undesired_datapt[0][1]+0.002, 'NH')  # Label 'NH' (e.g., Non-Healthy)\n",
    "        plt.scatter(optimal_datapt[0][0], optimal_datapt[0][1], c = 'g', label=\"H: Healthy\")  # Plot optimal point (changed to green for distinction)\n",
    "        plt.text(optimal_datapt[0][0]+0.002, optimal_datapt[0][1]+0.002, 'NH')  # Label 'H' (e.g., Healthy; adjusted from duplicate 'NH')\n",
    "        plt.plot([undesired_datapt[0][0], optimal_datapt[0][0]], [undesired_datapt[0][1],optimal_datapt[0][1]], linestyle='--')  # Dashed line between points\n",
    "        red_patch = mpatches.Patch(color='red', label='Not Healthy')\n",
    "        blue_patch = mpatches.Patch(color='blue', label='Healthy')\n",
    "        green_patch = mpatches.Patch(color='green', label=\"Counterfactual\")\n",
    "        purple_patch = mpatches.Patch(color='purple', label='Decision Boundary Point')\n",
    "        plt.legend(loc='lower left', handles=[red_patch, blue_patch, purple_patch, green_patch])\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.title(\"Toy Dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "    categorical_features = [col for col in inv_col_map.keys()]\n",
    "    final_optimal_datapt = [] \n",
    "\n",
    "    for col in X_train.columns:\n",
    "        if col in categorical_features: \n",
    "            idx = optimal_datapt[0,X_train.columns.get_loc(col)].astype(int)\n",
    "            final_optimal_datapt.append(inv_col_map[col][idx])\n",
    "        else: \n",
    "            final_optimal_datapt.append(optimal_datapt[0,X_train.columns.get_loc(col)])\n",
    "\n",
    "    query_instance = undesired_coords\n",
    "    return dataset, model, query_instance, final_optimal_datapt, euclidean_distance(undesired_datapt, optimal_datapt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021abd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dice_cfs(df, model, query_instance, method, continuous_features, target, chosen_row, plot=False, total_CFs=1):\n",
    "    start = datetime.datetime.now()\n",
    "    x_train = df.iloc[:, :-1]\n",
    "    backend = 'sklearn'\n",
    "\n",
    "    d = dice_ml.Data(dataframe=df, continuous_features=continuous_features, outcome_name=target)\n",
    "    m = dice_ml.Model(model=model, backend=backend)\n",
    "    exp_dice = dice_ml.Dice(d, m, method=method)\n",
    "    \n",
    "    sampled_dataset = df.sample(n=min(df.shape[0], 10000))\n",
    "\n",
    "    # Extract new training features/labels after balancing\n",
    "    X_train = sampled_dataset.iloc[:, :-1]\n",
    "    y_train = sampled_dataset.iloc[:, -1]\n",
    "    # Train the model\n",
    "    print(\"Fitting model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    print(\"QUERY INSTANCE\")\n",
    "    query_instance = x_train.iloc[[chosen_row]] \n",
    "\n",
    "    dice_cfs = exp_dice.generate_counterfactuals(pd.DataFrame(data=query_instance, columns=x_train.columns), total_CFs=total_CFs, desired_class=\"opposite\")\n",
    "    cfs_list = json.loads(dice_cfs.to_json())['cfs_list']\n",
    "\n",
    "    query_instance = np.array(query_instance)\n",
    "    dist_cfs = []\n",
    "    for point in cfs_list[0]: \n",
    "        point_vec = [float(point[i]) for i in range(len(point[:-1]))]\n",
    "        print(euclidean_distance(np.array(point_vec), query_instance))\n",
    "        dist_cfs.append(euclidean_distance(np.array(point_vec), query_instance))\n",
    "\n",
    "    # average_distance_cfs /= len(cfs_list[0])\n",
    "    # print(\"AVERAGE EUCLIDEAN DISTANCE AMONG 100 CFS\", average_distance_cfs)\n",
    "\n",
    "    if plot:\n",
    "        for point in cfs_list[0][:5]: \n",
    "            print(point)\n",
    "            x,y = point[0], point[1] \n",
    "            print(\"EUCLIDEAN DISTANCE:\", euclidean_distance(np.array((x,y)), query_instance))\n",
    "            print(query_instance)\n",
    "            plt.scatter(x,y, c = 'yellow')  # Plot optimal point (changed to green for distinction)\n",
    "            plt.text(x+0.002, y+0.002, 'H')  # Label 'H' (e.g., Healthy; adjusted from duplicate 'NH')\n",
    "            plt.plot([x,query_instance[0][0]], [y, query_instance[0][1]], linestyle='--')  # Dashed line between points\n",
    "    end = datetime.datetime.now()\n",
    "    diff = end - start\n",
    "    print(f\"Elapsed time: {diff}\")\n",
    "\n",
    "    return dist_cfs, diff.total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d4aab4",
   "metadata": {},
   "source": [
    "# Step 3: Toy Dataset Experiments\n",
    "\n",
    "We run a few experiments using the toy dataset, and we compare the results visually using both the optimal point method and the dice model-agnostic methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df.iloc[:,:-1]\n",
    "continuous_features=['x1', 'x2']\n",
    "target='y'\n",
    "query_instance=x_train.iloc[4:5,:]\n",
    "chosen_row=4\n",
    "visualize_dataset()\n",
    "df, model, query_instance, opt_point, _ = optimal_point(df, svm_classifier, desired_class=1, original_class=-1, threshold=100, chosen_row=chosen_row, point_epsilon=1e-6, epsilon=0.07, constraints=[], plot=True)\n",
    "run_dice_cfs(df=df, model=model,query_instance=query_instance,method='kdtree', continuous_features=continuous_features, target=target, chosen_row=chosen_row, total_CFs=5, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc3146",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df.iloc[:,:-1]\n",
    "continuous_features=['x1', 'x2']\n",
    "target='y'\n",
    "query_instance=x_train.iloc[4:5,:]\n",
    "chosen_row=4\n",
    "visualize_dataset()\n",
    "df, model, query_instance, opt_point,_ = optimal_point(df, svm_classifier, desired_class=1, original_class=-1, threshold=100, chosen_row=chosen_row, point_epsilon=1e-6, epsilon=0.07, constraints=[], plot=True)\n",
    "run_dice_cfs(df=df, model=model,query_instance=query_instance,method='random', continuous_features=continuous_features, target=target, chosen_row=chosen_row, total_CFs=5, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5476555",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df.iloc[:,:-1]\n",
    "continuous_features=['x1', 'x2']\n",
    "target='y'\n",
    "query_instance=x_train.iloc[4:5,:]\n",
    "chosen_row=4\n",
    "visualize_dataset()\n",
    "df, model, query_instance, opt_point,_ = optimal_point(df, svm_classifier, desired_class=1, original_class=-1, threshold=100, chosen_row=chosen_row, point_epsilon=1e-6, epsilon=0.07, constraints=[], plot=True)\n",
    "run_dice_cfs(df=df, model=model,query_instance=query_instance,method='genetic', continuous_features=continuous_features, target=target, chosen_row=chosen_row, total_CFs=5, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217feb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df.iloc[:,:-1]\n",
    "continuous_features=['x1', 'x2']\n",
    "target='y'\n",
    "query_instance=x_train.iloc[5:6,:]\n",
    "chosen_row=5\n",
    "visualize_dataset()\n",
    "df, model, query_instance, opt_point,_= optimal_point(df, svm_classifier, desired_class=1, original_class=-1, threshold=100, chosen_row=chosen_row, point_epsilon=1e-6, epsilon=0.07, constraints=[], plot=True)\n",
    "run_dice_cfs(df=df, model=model,query_instance=query_instance,method='kdtree', continuous_features=continuous_features, target=target, chosen_row=chosen_row, total_CFs=5, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdf823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df.iloc[:,:-1]\n",
    "continuous_features=['x1', 'x2']\n",
    "target='y'\n",
    "query_instance=x_train.iloc[5:6,:]\n",
    "chosen_row=5\n",
    "visualize_dataset()\n",
    "df, model, query_instance, opt_point, _ = optimal_point(df, svm_classifier, desired_class=1, original_class=-1, threshold=100, chosen_row=chosen_row, point_epsilon=1e-6, epsilon=0.07, constraints=[], plot=True)\n",
    "run_dice_cfs(df=df, model=model,query_instance=query_instance,method='random', continuous_features=continuous_features, target=target, chosen_row=chosen_row, total_CFs=5, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884e2ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df.iloc[:,:-1]\n",
    "continuous_features=['x1', 'x2']\n",
    "target='y'\n",
    "query_instance=x_train.iloc[5:6,:]\n",
    "chosen_row=5\n",
    "visualize_dataset()\n",
    "df, model, query_instance, opt_point, _ = optimal_point(df, svm_classifier, desired_class=1, original_class=-1, threshold=100, chosen_row=chosen_row, point_epsilon=1e-6, epsilon=0.07, constraints=[], plot=True)\n",
    "run_dice_cfs(df=df, model=model,query_instance=query_instance,method='genetic', continuous_features=continuous_features, target=target, chosen_row=chosen_row, total_CFs=5, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bca57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_map = { \n",
    "    1: -1, \n",
    "    -1: 1\n",
    "}\n",
    "x_train = df.iloc[:,:-1]\n",
    "y_train  = df.iloc[:,-1]\n",
    "continuous_features=['x1', 'x2']\n",
    "target='y'\n",
    "dice_dists, optimal_dists = [], [] \n",
    "for i in range(df.shape[0]):\n",
    "    chosen_row=i\n",
    "    query_instance=x_train.iloc[chosen_row:chosen_row+1,:]\n",
    "    label = y_train.iloc[chosen_row:chosen_row+1]\n",
    "    df, model, query_instance, opt_point, dist = optimal_point(df, svm_classifier, desired_class=inv_map[label.item()], original_class=label.item(), threshold=1000, chosen_row=chosen_row, point_epsilon=1e-6, epsilon=0.07, constraints=[], plot=False)\n",
    "    optimal_dists.append(dist)\n",
    "    dist_cfs, _ = run_dice_cfs(df=df, model=model,query_instance=query_instance,method='kdtree', continuous_features=continuous_features, target=target, chosen_row=chosen_row, total_CFs=100, plot=False)\n",
    "    dice_dists.extend(dist_cfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f458e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e136c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_map = { \n",
    "    1: -1, \n",
    "    -1: 1\n",
    "}\n",
    "x_train = df.iloc[:,:-1]\n",
    "y_train  = df.iloc[:,-1]\n",
    "continuous_features=['x1', 'x2']\n",
    "target='y'\n",
    "dice_dists, optimal_dists = [], [] \n",
    "for i in range(df.shape[0]):\n",
    "    chosen_row=i\n",
    "    query_instance=x_train.iloc[chosen_row:chosen_row+1,:]\n",
    "    label = y_train.iloc[chosen_row:chosen_row+1]\n",
    "    df, model, query_instance, opt_point, dist = optimal_point(df, svm_classifier, desired_class=inv_map[label.item()], original_class=label.item(), threshold=1000, chosen_row=chosen_row, point_epsilon=1e-6, epsilon=0.07, constraints=[], plot=False)\n",
    "    optimal_dists.append(dist)\n",
    "    dist_cfs, _ = run_dice_cfs(df=df, model=model,query_instance=query_instance,method='random', continuous_features=continuous_features, target=target, chosen_row=chosen_row, total_CFs=100, plot=False)\n",
    "    dice_dists.extend(dist_cfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d4e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fed399",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_map = { \n",
    "    1: -1, \n",
    "    -1: 1\n",
    "}\n",
    "x_train = df.iloc[:,:-1]\n",
    "y_train  = df.iloc[:,-1]\n",
    "continuous_features=['x1', 'x2']\n",
    "target='y'\n",
    "dice_dists, optimal_dists = [], [] \n",
    "for i in range(df.shape[0]):\n",
    "    chosen_row=i\n",
    "    query_instance=x_train.iloc[chosen_row:chosen_row+1,:]\n",
    "    label = y_train.iloc[chosen_row:chosen_row+1]\n",
    "    df, model, query_instance, opt_point, dist = optimal_point(df, svm_classifier, desired_class=inv_map[label.item()], original_class=label.item(), threshold=100, chosen_row=chosen_row, point_epsilon=1e-6, epsilon=0.07, constraints=[], plot=False)\n",
    "    optimal_dists.append(dist)\n",
    "    dist_cfs, _ = run_dice_cfs(df=df, model=model,query_instance=query_instance,method='genetic', continuous_features=continuous_features, target=target, chosen_row=chosen_row, total_CFs=100, plot=False)\n",
    "    dice_dists.extend(dist_cfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbf6ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dda666",
   "metadata": {},
   "source": [
    "# Step 3: Adult Income Dataset Experiments\n",
    "\n",
    "We run a few experiments using the adult income dataset comparing DiCE model-agnostic methodologies and the Optimal Point method.\n",
    "\n",
    "We follow the following steps: \n",
    "\n",
    "1. Import the dataset using the helpers function from DiCE \n",
    "2. Initialize the classifier which is a Random Forest Classifier in this case\n",
    "3. Iterate for 50 or 100 randomly selected points using the Optimal point method \n",
    "4. After each iteration of generation with the optimal point method, we apply the run_dice_cfs method that enables us to generate counterfactuals using DiCE's specific model-agnostic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adda3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = helpers.load_adult_income_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b3ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fdf419",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = dataset.iloc[:,:-1]\n",
    "y_train = dataset.iloc[:,-1]\n",
    "continuous_features=[\"age\", \"hours_per_week\"]\n",
    "target='income'\n",
    "query_instance=x_train.iloc[4:5,:]\n",
    "inv_map = { \n",
    "    0: 1, \n",
    "    1: 0\n",
    "}\n",
    "dice_dists, optimal_dists = [], [] \n",
    "sub_dataset = dataset[dataset['income'] == 0]\n",
    "random_integers = random.sample(range(0, sub_dataset.shape[0]-1), 50)\n",
    "for i in random_integers:\n",
    "    real_idx = sub_dataset.index[i]\n",
    "    chosen_row=real_idx\n",
    "    query_instance=x_train.iloc[chosen_row:chosen_row+1,:]\n",
    "    label = y_train.iloc[chosen_row:chosen_row+1]\n",
    "    df, model, query_instance, opt_point, dist = optimal_point(dataset, clf, desired_class=inv_map[label.item()], original_class=label.item(), threshold=10000, chosen_row=chosen_row, point_epsilon=1e-3, epsilon=0.01, constraints=[])\n",
    "    optimal_dists.append(dist)\n",
    "    dist_cfs, _ = run_dice_cfs(df=df, model=model,query_instance=query_instance,method='kdtree', continuous_features=continuous_features, target=target, chosen_row=chosen_row)\n",
    "    dice_dists.extend(dist_cfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4996a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e73a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = dataset.iloc[:,:-1]\n",
    "y_train = dataset.iloc[:,-1]\n",
    "continuous_features=[\"age\", \"hours_per_week\"]\n",
    "target='income'\n",
    "query_instance=x_train.iloc[4:5,:]\n",
    "inv_map = { \n",
    "    0: 1, \n",
    "    1: 0\n",
    "}\n",
    "dice_dists, optimal_dists = [], [] \n",
    "sub_dataset = dataset[dataset['income'] == 0]\n",
    "random_integers = random.sample(range(0, sub_dataset.shape[0]-1), 50)\n",
    "for i in random_integers:\n",
    "    real_idx = sub_dataset.index[i]\n",
    "    chosen_row=real_idx\n",
    "    query_instance=x_train.iloc[chosen_row:chosen_row+1,:]\n",
    "    label = y_train.iloc[chosen_row:chosen_row+1]\n",
    "    df, model, query_instance, opt_point, dist = optimal_point(dataset, clf, desired_class=inv_map[label.item()], original_class=label.item(), threshold=10000, chosen_row=chosen_row, point_epsilon=1e-3, epsilon=0.01, constraints=[])\n",
    "    optimal_dists.append(dist)\n",
    "    dist_cfs, _ = run_dice_cfs(df=df, model=model,query_instance=query_instance,method='random', continuous_features=continuous_features, target=target, chosen_row=chosen_row)\n",
    "    dice_dists.extend(dist_cfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fe2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad741871",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = dataset.iloc[:,:-1]\n",
    "y_train = dataset.iloc[:,-1]\n",
    "continuous_features=[\"age\", \"hours_per_week\"]\n",
    "target='income'\n",
    "query_instance=x_train.iloc[4:5,:]\n",
    "inv_map = { \n",
    "    0: 1, \n",
    "    1: 0\n",
    "}\n",
    "dice_dists, optimal_dists = [], [] \n",
    "sub_dataset = dataset[dataset['income'] == 0]\n",
    "random_integers = random.sample(range(0, sub_dataset.shape[0]-1), 50)\n",
    "for i in random_integers:\n",
    "    real_idx = sub_dataset.index[i]\n",
    "    chosen_row=real_idx\n",
    "    query_instance=x_train.iloc[chosen_row:chosen_row+1,:]\n",
    "    label = y_train.iloc[chosen_row:chosen_row+1]\n",
    "    df, model, query_instance, opt_point, dist = optimal_point(dataset, clf, desired_class=inv_map[label.item()], original_class=label.item(), threshold=10000, chosen_row=chosen_row, point_epsilon=1e-3, epsilon=0.01, constraints=[])\n",
    "    optimal_dists.append(dist)\n",
    "    dist_cfs, _ = run_dice_cfs(df=df, model=model,query_instance=query_instance,method='genetic', continuous_features=continuous_features, target=target, chosen_row=chosen_row)\n",
    "    dice_dists.extend(dist_cfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cde62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779cbe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf0e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = dataset.iloc[:,:-1]\n",
    "y_train = dataset.iloc[:,-1]\n",
    "continuous_features=[\"age\", \"hours_per_week\"]\n",
    "target='income'\n",
    "inv_map = { \n",
    "    0: 1, \n",
    "    1: 0\n",
    "}\n",
    "dice_dists, optimal_dists = [], [] \n",
    "random_integers = random.sample(range(1, 26047), 50)\n",
    "\n",
    "for i in random_integers:\n",
    "    chosen_row=i\n",
    "    query_instance=x_train.iloc[chosen_row:chosen_row+1,:]\n",
    "    label = y_train.iloc[chosen_row:chosen_row+1]\n",
    "    df, model, query_instance, opt_point, dist = optimal_point(dataset, clf, desired_class=inv_map[label.item()], original_class=label.item(), threshold=5000, chosen_row=chosen_row, point_epsilon=1e-3, epsilon=0.01, constraints=[])\n",
    "    optimal_dists.append(dist)\n",
    "    dist_cfs, _ = run_dice_cfs(df=df, model=model,query_instance=query_instance,method='kdtree', continuous_features=continuous_features, target=target, chosen_row=chosen_row)\n",
    "    dice_dists.extend(dist_cfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5cdd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39df6465",
   "metadata": {},
   "source": [
    "# Step 3: Heart Disease Dataset Experiments\n",
    "\n",
    "We run a few experiments using the heart disease dataset comparing DiCE model-agnostic methodologies and the Optimal Point method.\n",
    "\n",
    "We follow the following steps: \n",
    "\n",
    "1. Import the dataset using the helpers function from DiCE \n",
    "2. Initialize the classifier which is a Random Forest Classifier in this case\n",
    "3. Iterate for 50 or 100 randomly selected points using the Optimal point method \n",
    "4. After each iteration of generation with the optimal point method, we apply the run_dice_cfs method that enables us to generate counterfactuals using DiCE's specific model-agnostic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb7e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease = pd.read_csv(\n",
    "'../../heart.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96881f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b14a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = heart_disease.iloc[:,:-1]\n",
    "y_train = heart_disease.iloc[:,-1]\n",
    "continuous_features=[\"age\", \"trestbps\", \"thalach\", \"oldpeak\", \"chol\"]\n",
    "target='target'\n",
    "inv_map = { \n",
    "    0: 1, \n",
    "    1: 0\n",
    "}\n",
    "dice_dists, optimal_dists = [], [] \n",
    "dice_probs, optimal_probs = [], [] \n",
    "sub_dataset = heart_disease[heart_disease['target'] == 1]\n",
    "\n",
    "random_integers = random.sample(range(0, sub_dataset.shape[0]-1), 100)\n",
    "for i in random_integers:\n",
    "    real_idx = sub_dataset.index[i]\n",
    "    chosen_row=real_idx\n",
    "    query_instance=x_train.iloc[chosen_row:chosen_row+1,:]\n",
    "    df, model, query_instance, opt_point, dist = optimal_point(heart_disease, svm_classifier, desired_class=inv_map[label.item()], original_class=label.item(), threshold=5000, chosen_row=chosen_row, point_epsilon=1e-3, epsilon=0.01, constraints=[])\n",
    "    optimal_dists.append(dist)\n",
    "    dist_cfs,_ = run_dice_cfs(df=df, model=model,query_instance=query_instance,method='kdtree', continuous_features=continuous_features, target=target, chosen_row=chosen_row)\n",
    "    dice_dists.extend(dist_cfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a93d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = heart_disease.iloc[:,:-1]\n",
    "y_train = heart_disease.iloc[:,-1]\n",
    "continuous_features=[\"age\", \"trestbps\", \"thalach\", \"oldpeak\", \"chol\"]\n",
    "target='target'\n",
    "inv_map = { \n",
    "    0: 1, \n",
    "    1: 0\n",
    "}\n",
    "dice_dists, optimal_dists = [], [] \n",
    "dice_probs, optimal_probs = [], [] \n",
    "sub_dataset = heart_disease[heart_disease['target'] == 1]\n",
    "\n",
    "random_integers = random.sample(range(0, sub_dataset.shape[0]-1), 100)\n",
    "for i in random_integers:\n",
    "    real_idx = sub_dataset.index[i]\n",
    "    chosen_row=real_idx\n",
    "    query_instance=x_train.iloc[chosen_row:chosen_row+1,:]\n",
    "    df, model, query_instance, opt_point, dist = optimal_point(heart_disease, svm_classifier, desired_class=inv_map[label.item()], original_class=label.item(), threshold=10000, chosen_row=chosen_row, point_epsilon=1e-4, epsilon=0.001, constraints=[])\n",
    "    optimal_dists.append(dist)\n",
    "    dist_cfs, _ = run_dice_cfs(df=df, model=model,query_instance=query_instance,method='random', continuous_features=continuous_features, target=target, chosen_row=chosen_row)\n",
    "    dice_dists.extend(dist_cfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a163fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f8329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = heart_disease.iloc[:,:-1]\n",
    "y_train = heart_disease.iloc[:,-1]\n",
    "continuous_features=[\"age\", \"trestbps\", \"thalach\", \"oldpeak\", \"chol\"]\n",
    "target='target'\n",
    "inv_map = { \n",
    "    0: 1, \n",
    "    1: 0\n",
    "}\n",
    "dice_dists, optimal_dists = [], [] \n",
    "dice_probs, optimal_probs = [], [] \n",
    "sub_dataset = heart_disease[heart_disease['target'] == 1]\n",
    "\n",
    "random_integers = random.sample(range(0, sub_dataset.shape[0]-1), 100)\n",
    "for i in random_integers:\n",
    "    real_idx = sub_dataset.index[i]\n",
    "    chosen_row=real_idx\n",
    "    query_instance=x_train.iloc[chosen_row:chosen_row+1,:]\n",
    "    df, model, query_instance, opt_point, dist = optimal_point(heart_disease, svm_classifier, desired_class=inv_map[label.item()], original_class=label.item(), threshold=5000, chosen_row=chosen_row, point_epsilon=1e-3, epsilon=0.01, constraints=[])\n",
    "    optimal_dists.append(dist)\n",
    "    dist_cfs, _ = run_dice_cfs(df=df, model=model,query_instance=query_instance,method='genetic', continuous_features=continuous_features, target=target, chosen_row=chosen_row)\n",
    "    dice_dists.extend(dist_cfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2486f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6830972b",
   "metadata": {},
   "source": [
    "# Step 4: Runtime Tests \n",
    "\n",
    "The function ```runtime_tests()``` are used for comparing DiCE's model-agnostic approaches and Optimal Point for time complexity. We use a logistic regression classifier for examining runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c997f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runtime_tests(number_of_features, method, total_random=100):\n",
    "    X, y = make_classification(n_samples=5000, n_features=number_of_features, n_informative=number_of_features,\n",
    "                            n_redundant=0, n_classes=2, random_state=42)\n",
    "    y = np.reshape(y, (-1, 1))\n",
    "    columns = [\"x\"+str(i) for i in range(1, X.shape[1]+1)] \n",
    "    columns.append('y')\n",
    "    dataset = pd.DataFrame(data=np.hstack((X,y)), columns=columns)\n",
    "    continuous_features=[\"x\"+str(i) for i in range(1, X.shape[1]+1)]\n",
    "    target='y'\n",
    "    inv_map = { \n",
    "        0: 1, \n",
    "        1: 0\n",
    "    }\n",
    "    dice_dists, optimal_dists = [], []\n",
    "    dice_runtime = [] \n",
    "    sub_dataset = dataset[dataset[target] == 0]\n",
    "    random_integers = random.sample(range(1, sub_dataset.shape[0]), total_random)\n",
    "    clf = LogisticRegression()\n",
    "\n",
    "    for i in random_integers:\n",
    "        real_idx = sub_dataset.index[i]\n",
    "        chosen_row=real_idx\n",
    "        query_instance=X[chosen_row:chosen_row+1,:]\n",
    "        label = y[chosen_row:chosen_row+1]\n",
    "        df, model, query_instance, opt_point, dist = optimal_point(dataset, clf, desired_class=inv_map[label.item()], original_class=label.item(), threshold=5000, chosen_row=chosen_row, point_epsilon=1e-3, epsilon=0.01, constraints=[])\n",
    "        optimal_dists.append(dist)\n",
    "        dist_cfs, total_seconds = run_dice_cfs(df=df, model=model,query_instance=query_instance,method=method, continuous_features=continuous_features, target=target, chosen_row=chosen_row)\n",
    "        dice_dists.extend(dist_cfs)\n",
    "        dice_runtime.append(total_seconds)\n",
    "\n",
    "    print(np.mean(dice_runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9c5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_tests(number_of_features=10, method='kdtree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c41bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_tests(number_of_features=50, method='kdtree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f5df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_tests(number_of_features=10, method='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_tests(number_of_features=50, method='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_tests(number_of_features=10, method='genetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a385e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_tests(number_of_features=50, method='genetic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
