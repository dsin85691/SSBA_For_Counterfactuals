{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0b26986",
   "metadata": {},
   "source": [
    "# DiCE ML Comparison with Optimal Point Method \n",
    "\n",
    "We compare the DiCE Model-agnostic methods with the optimal point method in this notebook. \n",
    "\n",
    "1. First, we import DiCE ML model-agnostic methods \n",
    "2. Second, we import the packaged files needed to run the \"Optimal Point\" Methodology \n",
    "3. Third, we run the experiments and compare the results at the end using different models such as SVM and random forest classifier.\n",
    "4. Finally, we compare the runtimes of DiCE and the Optimal Point methodology\n",
    "\n",
    "Note: Running experiments for the adult income can take hours. Please be mindful of the runtime for these experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beef340",
   "metadata": {},
   "source": [
    "# Step 1: Importing DiCE ML and helper functions \n",
    "\n",
    "Below we import DiCE ML and their relevant helper functions. We import the sci-kit learn library and some of their necessary methods to make sure that we can run the experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import DiCE\n",
    "import dice_ml\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import ssl\n",
    "import json\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "from dice_ml.utils import helpers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7874df33",
   "metadata": {},
   "source": [
    "# Step 2: Import the necessary functionality to make Optimal Point methodology work \n",
    "\n",
    "We import many of the methods needed for the ```optimal_point()``` function to work as intended below. We import additional methods from ```binary_search_optimal_point().```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a8f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from files.common_functions import euclidean_distance, closest_border_point, closest_point, move_from_A_to_B_with_x1_displacement\n",
    "from files.common_functions import get_multi_dim_border_points, det_constraints, real_world_constraints, constraint_bounds\n",
    "from files.common_functions import balance_dataset, check_class_balance, convert_columns\n",
    "from files.binary_search_optimal_point import multi_decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb59680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading cuML GPU acceleration library\n",
    "%load_ext cuml.accel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54229082",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../../toy_dataset.csv')\n",
    "# SVM classifier with polynomial decision boundary\n",
    "svm_classifier = svm.SVC(kernel='poly',C=10, degree=2, probability=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf44886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_point(dataset, model, desired_class, original_class, chosen_row=-1, threshold=10000, point_epsilon=0.1, epsilon=0.01, constraints=[], delta=15, plot=False, step=0.5):\n",
    "    \"\"\"\n",
    "    Finds the closest point to the decision boundary from an undesired point,\n",
    "    optionally constrained by real-world conditions.\n",
    "    This essentially finds the counterfactual explanation for a given point by minimizing the distance to the given boundary.\n",
    "    This method is important because it addresses a key problem with the original optimal_point() function where we generated an R^n dimensional grid that we would then have to iterate over.\n",
    "    The problem with iterating over such a grid is eventually that we will hit a memory error for high-dimensional features such as 20, 30 or 40 features. This will cause the function to crash.\n",
    "    Additionally, due to the exponential increase of the number of features to search, the grid will become infeasible to search (curse of dimensionality).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pd.DataFrame\n",
    "        Full dataset containing features and a final column with class labels.\n",
    "\n",
    "    model : sklearn-like classifier\n",
    "        A binary classification model with a `.fit()` and `.predict()` method.\n",
    "\n",
    "    desired_class : int or label\n",
    "        The target class we want the corrected point to belong to.\n",
    "\n",
    "    original_class : int or label\n",
    "        The actual class label of the undesired point.\n",
    "\n",
    "    chosen_row :  int\n",
    "        The selected row of the dataset to find the counterfactual explanation for\n",
    "\n",
    "    threshold : int, optional\n",
    "        Max number of decision boundary points to sample. Default is 10000.\n",
    "\n",
    "    point_epsilon : float, optional\n",
    "        Precision used to estimate decision boundary points. Default is 0.1.\n",
    "\n",
    "    epsilon : float, optional\n",
    "        Step size used when displacing a point toward the decision boundary. Default is 0.01.\n",
    "\n",
    "    constraints : list, optional\n",
    "        A list of real-world constraints on the features (e.g., ranges, logic constraints). Default is [].\n",
    "\n",
    "    delta : int, optional\n",
    "        Tolerances or maximum displacement for each continuous feature\n",
    "\n",
    "    plot : boolean\n",
    "        Used as a parameter to determine whether to plot the results or not\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        A corrected point that satisfies the class change and real-world constraints.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    Exception\n",
    "        If the number of constraints exceeds the number of features.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This function trains the model on the provided dataset, generates boundary points using\n",
    "      `find_decision_boundary`, applies constraints, and finds the closest optimal point.\n",
    "    - Assumes binary classification and relies on external functions like `real_world_constraints`,\n",
    "      `closest_point`, `move_from_A_to_B_with_x1_displacement`, etc., which must be defined elsewhere.\n",
    "    - Includes plotting for visualization (e.g., boundary contours, points), which requires matplotlib.\n",
    "    - The function blends boundary approximation with counterfactual generation, useful for explainable AI.\n",
    "    - Print statements are for progress tracking; plotting is partially commented out but can be enabled.\n",
    "    - Usage: Call with a dataset and model to generate counterfactuals, e.g., for model interpretation or optimization.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> from sklearn.linear_model import LogisticRegression\n",
    "    >>> dataset = pd.DataFrame({'feat1': [0, 1, 2], 'feat2': [0, 1, 0], 'label': [0, 1, 0]})\n",
    "    >>> model = LogisticRegression()\n",
    "    >>> undesired_coords = [2, 0]  # Example point from class 0\n",
    "    >>> optimal = optimal_point(dataset, model, desired_class=1, original_class=0, undesired_coords=undesired_coords)\n",
    "    >>> print(optimal)  # e.g., array([[1.5, 0.5]])\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert categorical columns if needed (before balancing)\n",
    "    inv_col_map = convert_columns(dataset)\n",
    "\n",
    "    # Extract features and labels before balancing\n",
    "    X_orig = dataset.iloc[:, :-1]\n",
    "\n",
    "    # Save the original row's feature values\n",
    "    undesired_coords = X_orig.iloc[chosen_row, :].copy()\n",
    "\n",
    "    # Balance the dataset\n",
    "    dataset = balance_dataset(df=dataset, target=dataset.columns[-1])\n",
    "\n",
    "    if not check_class_balance(dataset, target=dataset.columns[-1]):\n",
    "        raise RuntimeError(\"Failed to balance classes for binary classification\")\n",
    "\n",
    "    sampled_dataset = dataset.sample(n=min(dataset.shape[0], 20000))\n",
    "\n",
    "    # Extract new training features/labels after balancing\n",
    "    X_train = sampled_dataset.iloc[:, :-1]\n",
    "    y_train = sampled_dataset.iloc[:, -1]\n",
    "    # Train the model\n",
    "    print(\"Fitting model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # STEP 2: Find decision boundary\n",
    "    # -------------------------------\n",
    "    print(\"boundary points started generation...\")\n",
    "\n",
    "    # This step uses binary interpolation to get points close to the decision boundary\n",
    "    boundary_points = multi_decision_boundary(model, X_train, y_train,\n",
    "                                             threshold=threshold, epsilon=point_epsilon)\n",
    "    print(\"boundary points finished.\")\n",
    "    print(boundary_points.shape)\n",
    "    # Detect categorical features (assumed as int columns)\n",
    "    categorical_features = X_train.select_dtypes(include=['int32', 'int64', 'int8']).columns.tolist()\n",
    "\n",
    "    # Round categoricals to int for discrete values\n",
    "    for col in categorical_features:\n",
    "        boundary_points[col] = boundary_points[col].astype(int)\n",
    "\n",
    "    # -------------------------------\n",
    "    # STEP 3: Apply real-world constraints (optional)\n",
    "    # -------------------------------\n",
    "    # Reduce boundary points based on external rules (e.g., cost limits, physics constraints)\n",
    "    contours_pd = real_world_constraints(points=boundary_points,\n",
    "                                      undesired_coords=undesired_coords,\n",
    "                                      constraints=constraints)\n",
    "    undesired_datapt = np.reshape(undesired_coords, (1, -1))  # Reshape undesired point to 2D array\n",
    "\n",
    "    # if plot:\n",
    "    #     plt.plot(contours[:,0], contours[:,1], lw=0.5, color='red')  # Commented: Plot contours for visualization\n",
    "\n",
    "    # -------------------------------\n",
    "    # STEP 4: Find closest point on constrained boundary\n",
    "     # -------------------------------\n",
    "    if contours_pd is not None and desired_class != original_class:\n",
    "        contours = contours_pd.to_numpy()\n",
    "        print(\"Finding the closest point from the contour line to the point...\")\n",
    "        contours_pd.reset_index(drop=True, inplace=True)\n",
    "        optimal_datapt = closest_point(undesired_datapt, contour=contours)\n",
    "        print(\"Found the closest point from the contour line to the point.\")  # Note: Duplicate print, possibly a typo\n",
    "        D = optimal_datapt - undesired_datapt  # Compute direction vector\n",
    "        deltas = D * (1+epsilon)  # Scale by (1 + epsilon) to overshoot\n",
    "        optimal_datapt = move_from_A_to_B_with_x1_displacement(undesired_datapt, optimal_datapt, deltas=deltas)\n",
    "    elif desired_class == original_class or contours_pd is None:\n",
    "        # If we want to *stay within* the same class (more constrained)\n",
    "        all_constrained_feats = [var for (var,_) in constraints]\n",
    "        closest_boundedpt = None\n",
    "        vars = set(X_train.columns) - set(all_constrained_feats)\n",
    "        cont_mutable_vars = [X_train.columns.get_loc(col) for col in vars]\n",
    "        deltas, len_constr = det_constraints(datapt=undesired_datapt[0], vars=cont_mutable_vars, deltas=deltas)  # Determine constraints\n",
    "\n",
    "        if len_constr > X_train.shape[1]:\n",
    "            raise Exception(\"There cannot be more constraints than features\")\n",
    "        else:\n",
    "            # All n dimensions are constrained, so generate an exact grid of boundary candidates\n",
    "            bounded_contour_pts = get_multi_dim_border_points(center=undesired_datapt[0],\n",
    "                                                              extents=deltas,\n",
    "                                                              step=0.1)\n",
    "            np_bounded_contour = np.array(bounded_contour_pts)  # Convert to NumPy array\n",
    "            if plot:\n",
    "                x_values, y_values = np_bounded_contour[:, 0], np_bounded_contour[:, 1]  # Extract x/y for plotting\n",
    "                plt.scatter(x_values, y_values, marker='o')  # Plot bounded points\n",
    "\n",
    "            closest_boundedpt = closest_border_point(np_bounded_contour, contour=boundary_points)  # Find closest on border\n",
    "            print(closest_boundedpt)\n",
    "        D = closest_boundedpt - undesired_datapt  # Compute direction\n",
    "        optimal_datapt = move_from_A_to_B_with_x1_displacement(undesired_datapt, closest_boundedpt, deltas=D)  # Move point\n",
    "    # Plot original and optimal points with connecting line\n",
    "    # if plot:\n",
    "    #     plt.scatter(undesired_datapt[0][0], undesired_datapt[0][1], c = 'r')  # Plot undesired point\n",
    "    #     plt.text(undesired_datapt[0][0]+0.002, undesired_datapt[0][1]+0.002, 'NH')  # Label 'NH' (e.g., Non-Healthy)\n",
    "    #     plt.scatter(optimal_datapt[0][0], optimal_datapt[0][1], c = 'g')  # Plot optimal point (changed to green for distinction)\n",
    "    #     plt.text(optimal_datapt[0][0]+0.002, optimal_datapt[0][1]+0.002, 'NH')  # Label 'H' (e.g., Healthy; adjusted from duplicate 'NH')\n",
    "    #     plt.plot([undesired_datapt[0][0], optimal_datapt[0][0]], [undesired_datapt[0][1],optimal_datapt[0][1]], linestyle='--')  # Dashed line between points\n",
    "\n",
    "    categorical_features = [col for col in inv_col_map.keys()]\n",
    "    final_optimal_datapt = []\n",
    "\n",
    "    for col in X_train.columns:\n",
    "        if col in categorical_features:\n",
    "            idx = int(optimal_datapt[0,X_train.columns.get_loc(col)])\n",
    "            final_optimal_datapt.append(inv_col_map[col][idx])\n",
    "        else:\n",
    "            final_optimal_datapt.append(optimal_datapt[0,X_train.columns.get_loc(col)])\n",
    "    query_instance = undesired_coords\n",
    "    return dataset, model, query_instance, final_optimal_datapt, euclidean_distance(undesired_datapt, optimal_datapt), delta, boundary_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404129ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp_vec_per_axis(v, ref_point, bool_vec, frac=0.05):\n",
    "    \"\"\"\n",
    "    Clamp displacement vector v per-axis so endpoint p0+v stays inside +/- frac*|p0_i| (plus eps).\n",
    "\n",
    "    Args:\n",
    "      v    : array-like shape (n,)\n",
    "      ref_point   : array-like shape (n,)\n",
    "      bool_vec : boolean vector for isolating categorical features out of changes\n",
    "      frac : fraction (default 0.05)\n",
    "\n",
    "    Returns:\n",
    "      a clipped vector that is bounded within half of the interval on both sides for each dimension\n",
    "    \"\"\"\n",
    "    # allowable bounds for the final point\n",
    "    lower_bounds = (ref_point - (frac/2) * np.abs(ref_point)) * bool_vec\n",
    "    upper_bounds = (ref_point + (frac/2) * np.abs(ref_point)) * bool_vec\n",
    "\n",
    "    # clamp the endpoint\n",
    "    endpoint = np.clip(ref_point + v, lower_bounds, upper_bounds)\n",
    "    return np.clip(endpoint, lower_bounds, upper_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021abd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dice_cfs(df, model, query_instance, method, continuous_features, categorical_features, target, chosen_row, contours, plot=False, total_CFs=1, delta=100):\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    x_train = df.iloc[:, :-1]\n",
    "    backend='sklearn'\n",
    "\n",
    "    d = dice_ml.Data(dataframe=df, continuous_features=continuous_features, categorical_features=categorical_features, outcome_name=target)\n",
    "    m = dice_ml.Model(model=model, backend=backend)\n",
    "\n",
    "    exp_dice = dice_ml.Dice(d, m, method=method)\n",
    "\n",
    "    query_instance = x_train.iloc[[chosen_row]].to_numpy()\n",
    "\n",
    "    dice_cfs = exp_dice.generate_counterfactuals(pd.DataFrame(data=query_instance, columns=x_train.columns),\n",
    "                                                        total_CFs=total_CFs, desired_class=\"opposite\")\n",
    "\n",
    "    cfs_list = json.loads(dice_cfs.to_json())['cfs_list']\n",
    "    dist_cfs = []\n",
    "\n",
    "    # np_bounded_contour = np.array(bounded_contour_pts)  # Convert to NumPy array\n",
    "    # x_values, y_values = np_bounded_contour[:, 0], np_bounded_contour[:, 1]  # Extract x/y for plotting\n",
    "    # if plot:\n",
    "    #     plt.scatter(x_values, y_values, marker='o')  # Plot bounded points\n",
    "    contours = contours.reset_index(drop=True)\n",
    "\n",
    "    bool_vec = []\n",
    "    for col in df.iloc[:, :-1].columns:\n",
    "        if np.issubdtype(df[col].dtype, np.number):\n",
    "            bool_vec.append(1)   # numeric -> allow changes\n",
    "        else:\n",
    "            bool_vec.append(0)   # categorical -> mask out\n",
    "\n",
    "    if delta == 100:\n",
    "        for point in cfs_list[0]:\n",
    "            point_vec = [float(point[i]) for i in range(len(point[:-1]))]\n",
    "            dist_cfs.append(euclidean_distance(np.array(point_vec), query_instance))\n",
    "    else:\n",
    "        for point in cfs_list[0]:\n",
    "            point_vec = [float(point[i]) for i in range(len(point[:-1]))]\n",
    "            point_vec = np.reshape(np.array(point_vec), (1, -1))\n",
    "            endpoint_vec = clamp_vec_per_axis(point_vec, ref_point=query_instance, bool_vec=bool_vec, frac=delta/100)\n",
    "            closest_pt = closest_point(endpoint_vec, contour=contours.to_numpy())\n",
    "            dist_cfs.append(euclidean_distance(closest_pt, endpoint_vec))\n",
    "\n",
    "    # if plot:\n",
    "    #     for point in cfs_list[0][:5]:\n",
    "    #         x,y = point[0], point[1]\n",
    "    #         print(\"EUCLIDEAN DISTANCE:\", euclidean_distance(delta*np.array((x,y)), query_instance))\n",
    "    #         plt.scatter(x,y, c = 'yellow')  # Plot optimal point (changed to green for distinction)\n",
    "    #         plt.text(x+0.002, y+0.002, 'H')  # Label 'H' (e.g., Healthy; adjusted from duplicate 'NH')\n",
    "    #         plt.plot([x,query_instance[0][0]], [y, query_instance[0][1]], linestyle='--')  # Dashed line between points\n",
    "    end = datetime.datetime.now()\n",
    "    diff = end - start\n",
    "    print(f\"Elapsed time: {diff}\")\n",
    "\n",
    "    return dist_cfs, diff.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ba61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exps(dataset, model, method, target, x_train, y_train, continuous_features, categorical_features, inv_map, num_samples, delta=100, constraints=[], threshold=25000):\n",
    "    dice_dists, optimal_dists = [], []\n",
    "    sub_dataset = dataset[dataset[target] == 1]\n",
    "    random_integers = random.sample(range(0, sub_dataset.shape[0]-1), num_samples)\n",
    "\n",
    "    for i in random_integers:\n",
    "        real_idx = sub_dataset.index[i]\n",
    "        chosen_row=real_idx\n",
    "        query_instance=x_train.iloc[chosen_row:chosen_row+1,:]\n",
    "        label = y_train.iloc[chosen_row:chosen_row+1]\n",
    "        df, model, query_instance, opt_point, dist, exp_delta, boundary_points = optimal_point(dataset, model, desired_class=0, original_class=1, threshold=threshold, chosen_row=chosen_row, point_epsilon=1e-3, epsilon=0.01, constraints=constraints, delta=delta, step=0.1)\n",
    "        optimal_dists.append(dist)\n",
    "        dist_cfs, _ = run_dice_cfs(df=df, model=model, query_instance=query_instance,method=method, continuous_features=continuous_features, categorical_features=categorical_features, target=target, contours=boundary_points, chosen_row=chosen_row, delta=exp_delta, total_CFs=10)\n",
    "        dice_dists.extend(dist_cfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d4aab4",
   "metadata": {},
   "source": [
    "# Step 3: Toy Dataset\n",
    "\n",
    "We run a few experiments using the toy dataset, and we compare the results visually using both the optimal point method and the dice model-agnostic methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400db22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_map = {\n",
    "    1: -1,\n",
    "    -1: 1\n",
    "}\n",
    "x_train = df.iloc[:,:-1]\n",
    "y_train  = df.iloc[:,-1]\n",
    "continuous_features=['x1', 'x2']\n",
    "categorical_features=[]\n",
    "target='y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bca57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dists, dice_dists = exps(df, model=svm_classifier, method='kdtree',\n",
    "                                 x_train=x_train, y_train=y_train,\n",
    "                                 continuous_features=continuous_features,\n",
    "                                 categorical_features=categorical_features,\n",
    "                                 inv_map=inv_map, num_samples=500,\n",
    "                                 target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f458e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e136c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dists, dice_dists = exps(df, model=svm_classifier, method='random',\n",
    "                                 x_train=x_train, y_train=y_train,\n",
    "                                 continuous_features=continuous_features,\n",
    "                                 categorical_features=categorical_features,\n",
    "                                 inv_map=inv_map, num_samples=500,\n",
    "                                 target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d4e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fed399",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dists, dice_dists = exps(df, model=svm_classifier, method='genetic',\n",
    "                                 x_train=x_train, y_train=y_train,\n",
    "                                 continuous_features=continuous_features,\n",
    "                                 categorical_features=categorical_features,\n",
    "                                 inv_map=inv_map, num_samples=500,\n",
    "                                 target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbf6ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dda666",
   "metadata": {},
   "source": [
    "# Step 3: Adult Income Dataset Experiments\n",
    "\n",
    "We run a few experiments using the adult income dataset comparing DiCE model-agnostic methodologies and the Optimal Point method.\n",
    "\n",
    "We follow the following steps: \n",
    "\n",
    "1. Import the dataset using the helpers function from DiCE \n",
    "2. Initialize the classifier which is a Random Forest Classifier in this case\n",
    "3. Iterate for 50 or 100 randomly selected points using the Optimal point method \n",
    "4. After each iteration of generation with the optimal point method, we apply the run_dice_cfs method that enables us to generate counterfactuals using DiCE's specific model-agnostic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3c1155",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = helpers.load_adult_income_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b3ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c921bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = dataset.iloc[:,:-1]\n",
    "y_train = dataset.iloc[:,-1]\n",
    "continuous_features=[\"age\", \"hours_per_week\"]\n",
    "categorical_features = ['marital_status', 'workclass', 'education', 'race', 'gender', 'occupation']\n",
    "target='income'\n",
    "inv_map = {\n",
    "    0: 1,\n",
    "    1: 0\n",
    "}\n",
    "constraints = [\n",
    "    (\"age\", \"equal\"),\n",
    "    (\"workclass\", \"equal\"),\n",
    "    (\"education\", \"equal\"),\n",
    "    (\"race\", \"equal\"),\n",
    "    (\"gender\", \"equal\"),\n",
    "    (\"occupation\", \"equal\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fdf419",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dists, dice_dists = exps(dataset, clf, 'kdtree', target=target,\n",
    "                                  x_train=x_train, y_train=y_train,\n",
    "                                  continuous_features=continuous_features,\n",
    "                                    categorical_features=categorical_features,\n",
    "                                    inv_map=inv_map, num_samples=500, delta=20,\n",
    "                                    constraints=constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4996a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e73a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dists, dice_dists = exps(dataset, clf, 'random', target=target,\n",
    "                                  x_train=x_train, y_train=y_train,\n",
    "                                  continuous_features=continuous_features,\n",
    "                                    categorical_features=categorical_features,\n",
    "                                    inv_map=inv_map, num_samples=500, delta=20,\n",
    "                                    constraints=constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fe2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad741871",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dists, dice_dists = exps(dataset, clf, 'genetic', target=target,\n",
    "                                  x_train=x_train, y_train=y_train,\n",
    "                                  continuous_features=continuous_features,\n",
    "                                    categorical_features=categorical_features,\n",
    "                                    inv_map=inv_map, num_samples=500, delta=20,\n",
    "                                    constraints=constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cde62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39df6465",
   "metadata": {},
   "source": [
    "# Step 3: Heart Disease Dataset Experiments\n",
    "\n",
    "We run a few experiments using the heart disease dataset comparing DiCE model-agnostic methodologies and the Optimal Point method.\n",
    "\n",
    "We follow the following steps: \n",
    "\n",
    "1. Import the dataset using the helpers function from DiCE \n",
    "2. Initialize the classifier which is a Random Forest Classifier in this case\n",
    "3. Iterate for 50 or 100 randomly selected points using the Optimal point method \n",
    "4. After each iteration of generation with the optimal point method, we apply the run_dice_cfs method that enables us to generate counterfactuals using DiCE's specific model-agnostic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb7e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease = pd.read_csv(\n",
    "'../../heart.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96881f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(heart_disease.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26c0db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = [\n",
    "    (\"age\", \"equal\"),\n",
    "    (\"sex\", \"equal\"),\n",
    "    (\"cp\", \"equal\"),\n",
    "    (\"fbs\", \"equal\"),\n",
    "    (\"restecg\", \"equal\"),\n",
    "    (\"exang\", \"equal\"),\n",
    "    (\"slope\", \"equal\"),\n",
    "    (\"thal\", \"equal\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b34b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = heart_disease.iloc[:,:-1]\n",
    "y_train = heart_disease.iloc[:,-1]\n",
    "continuous_features=[\"age\", \"trestbps\", \"thalach\", \"oldpeak\", \"chol\"]\n",
    "categorical_features=['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n",
    "target='target'\n",
    "inv_map = {\n",
    "    0: 1,\n",
    "    1: 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671bd6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dists, dice_dists = exps(dataset=heart_disease, model=svm_classifier, method='kdtree', target=target,\n",
    "                                  x_train=x_train, y_train=y_train,\n",
    "                                  continuous_features=continuous_features,\n",
    "                                    categorical_features=categorical_features,\n",
    "                                    inv_map=inv_map, num_samples=100, delta=20,\n",
    "                                    constraints=constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a93d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dists, dice_dists = exps(dataset=heart_disease, model=svm_classifier, method='random', target=target,\n",
    "                                  x_train=x_train, y_train=y_train,\n",
    "                                  continuous_features=continuous_features,\n",
    "                                    categorical_features=categorical_features,\n",
    "                                    inv_map=inv_map, num_samples=500, delta=20,\n",
    "                                    constraints=constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a163fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f8329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dists, dice_dists = exps(dataset=heart_disease, model=svm_classifier, method='genetic', target=target,\n",
    "                                  x_train=x_train, y_train=y_train,\n",
    "                                  continuous_features=continuous_features,\n",
    "                                    categorical_features=categorical_features,\n",
    "                                    inv_map=inv_map, num_samples=500, delta=20,\n",
    "                                    constraints=constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2486f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e3f48b",
   "metadata": {},
   "source": [
    "# Comparing DiCE with large feature dataset made of numerical features \n",
    "\n",
    "We use the ```make_classification``` function of sci-kit learn library to generate a synthetic dataset of numerical values that we can then compare both methodologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3bea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=2000, n_features=20, n_informative=20, n_redundant=0, random_state=42, n_classes=2)\n",
    "y = y.reshape(-1,1)\n",
    "columns = [\"x\"+str(i) for i in range(20)]\n",
    "columns.append('y')\n",
    "dataset = pd.DataFrame(data=np.hstack((X,y)), columns=columns)\n",
    "model = LogisticRegression()\n",
    "continuous_features = columns[:-1]\n",
    "categorical_features=[]\n",
    "target = 'y'\n",
    "x_train = dataset.iloc[:,:-1]\n",
    "y_train = dataset.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dists, dice_dists = exps(dataset=dataset, model=model,\n",
    "                                 method='kdtree', target=target,\n",
    "                                  x_train=x_train, y_train=y_train,\n",
    "                                  continuous_features=continuous_features,\n",
    "                                    categorical_features=categorical_features,\n",
    "                                    inv_map=inv_map, num_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efb540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a644f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dists, dice_dists = exps(dataset=dataset, model=model,\n",
    "                                 method='random', target=target,\n",
    "                                  x_train=x_train, y_train=y_train,\n",
    "                                  continuous_features=continuous_features,\n",
    "                                    categorical_features=categorical_features,\n",
    "                                    inv_map=inv_map, num_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51f0c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b1820",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dists, dice_dists = exps(dataset=dataset, model=model,\n",
    "                                 method='genetic', target=target,\n",
    "                                  x_train=x_train, y_train=y_train,\n",
    "                                  continuous_features=continuous_features,\n",
    "                                    categorical_features=categorical_features,\n",
    "                                    inv_map=inv_map, num_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af3fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6830972b",
   "metadata": {},
   "source": [
    "# Step 4: Runtime Tests \n",
    "\n",
    "The function ```runtime_tests()``` are used for comparing DiCE's model-agnostic approaches and Optimal Point for time complexity. We use a logistic regression classifier for examining runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c997f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runtime_tests(number_of_features, method, total_random=100):\n",
    "    X, y = make_classification(n_samples=5000, n_features=number_of_features, n_informative=number_of_features,\n",
    "                            n_redundant=0, n_classes=2, random_state=42)\n",
    "    y = np.reshape(y, (-1, 1))\n",
    "    columns = [\"x\"+str(i) for i in range(1, X.shape[1]+1)]\n",
    "    columns.append('y')\n",
    "    dataset = pd.DataFrame(data=np.hstack((X,y)), columns=columns)\n",
    "    continuous_features=[\"x\"+str(i) for i in range(1, X.shape[1]+1)]\n",
    "    target='y'\n",
    "    inv_map = {\n",
    "        0: 1,\n",
    "        1: 0\n",
    "    }\n",
    "    dice_dists, optimal_dists = [], []\n",
    "    dice_runtime = []\n",
    "    sub_dataset = dataset[dataset[target] == 0]\n",
    "    random_integers = random.sample(range(1, sub_dataset.shape[0]), total_random)\n",
    "    clf = LogisticRegression()\n",
    "\n",
    "    for i in random_integers:\n",
    "        real_idx = sub_dataset.index[i]\n",
    "        chosen_row=real_idx\n",
    "        query_instance=X[chosen_row:chosen_row+1,:]\n",
    "        label = y[chosen_row:chosen_row+1]\n",
    "        df, model, query_instance, opt_point, dist,_, contours = optimal_point(dataset, clf, desired_class=inv_map[label.item()], original_class=label.item(), threshold=5000, chosen_row=chosen_row, point_epsilon=1e-3, epsilon=0.01, constraints=[])\n",
    "        optimal_dists.append(dist)\n",
    "        dist_cfs, total_seconds = run_dice_cfs(df=df, contours=contours, model=model,query_instance=query_instance,method=method, continuous_features=continuous_features, categorical_features=[], target=target, chosen_row=chosen_row)\n",
    "        dice_dists.extend(dist_cfs)\n",
    "        dice_runtime.append(total_seconds)\n",
    "\n",
    "    print(np.mean(dice_runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9c5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_tests(number_of_features=10, method='kdtree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c41bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_tests(number_of_features=50, method='kdtree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f5df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_tests(number_of_features=10, method='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_tests(number_of_features=50, method='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_tests(number_of_features=10, method='genetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a385e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_tests(number_of_features=50, method='genetic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
