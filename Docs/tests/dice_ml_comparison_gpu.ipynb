{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0b26986",
   "metadata": {},
   "source": [
    "# DiCE ML Comparison with Optimal Point Method \n",
    "\n",
    "We compare the DiCE Model-agnostic methods with the optimal point method in this notebook. \n",
    "\n",
    "1. First, we import DiCE ML model-agnostic methods \n",
    "2. Second, we import the packaged files needed to run the \"Optimal Point\" Methodology \n",
    "3. Third, we run the experiments and compare the results at the end using different models such as SVM and random forest classifier.\n",
    "4. Finally, we compare the runtimes of DiCE and the Optimal Point methodology\n",
    "\n",
    "Note: Running experiments for the adult income can take hours. Please be mindful of the runtime for these experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beef340",
   "metadata": {},
   "source": [
    "# Step 1: Importing DiCE ML and helper functions \n",
    "\n",
    "Below we import DiCE ML and their relevant helper functions. We import the sci-kit learn library and some of their necessary methods to make sure that we can run the experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c0d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import DiCE\n",
    "import dice_ml\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "\n",
    "import ssl\n",
    "import json\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "from dice_ml.utils import helpers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7874df33",
   "metadata": {},
   "source": [
    "# Step 2: Import the necessary functionality to make Optimal Point methodology work \n",
    "\n",
    "We import many of the methods needed for the ```optimal_point()``` function to work as intended below. We import additional methods from ```binary_search_optimal_point().```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28a8f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from files.common_functions import euclidean_distance, closest_border_point, closest_point, move_from_A_to_B_with_x1_displacement\n",
    "from files.common_functions import get_multi_dim_border_points, det_constraints, real_world_constraints, constraint_bounds\n",
    "from files.common_functions import balance_dataset, check_class_balance, convert_columns\n",
    "from files.binary_search_optimal_point import multi_decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fb59680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading cuML GPU acceleration library\n",
    "%load_ext cuml.accel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54229082",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../../toy_dataset.csv')\n",
    "# SVM classifier with polynomial decision boundary\n",
    "svm_classifier = svm.SVC(kernel='poly',C=10, degree=2, probability=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cf44886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_point(dataset, model, desired_class, original_class, chosen_row=-1, threshold=10000, point_epsilon=0.1, epsilon=0.01, constraints=[], deltas=[], plot=False):\n",
    "    \"\"\"\n",
    "    Finds the closest point to the decision boundary from an undesired point,\n",
    "    optionally constrained by real-world conditions.\n",
    "    This essentially finds the counterfactual explanation for a given point by minimizing the distance to the given boundary.\n",
    "    This method is important because it addresses a key problem with the original optimal_point() function where we generated an R^n dimensional grid that we would then have to iterate over.\n",
    "    The problem with iterating over such a grid is eventually that we will hit a memory error for high-dimensional features such as 20, 30 or 40 features. This will cause the function to crash.\n",
    "    Additionally, due to the exponential increase of the number of features to search, the grid will become infeasible to search (curse of dimensionality).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pd.DataFrame\n",
    "        Full dataset containing features and a final column with class labels.\n",
    "\n",
    "    model : sklearn-like classifier\n",
    "        A binary classification model with a `.fit()` and `.predict()` method.\n",
    "\n",
    "    desired_class : int or label\n",
    "        The target class we want the corrected point to belong to.\n",
    "\n",
    "    original_class : int or label\n",
    "        The actual class label of the undesired point.\n",
    "\n",
    "    chosen_row :  int\n",
    "        The selected row of the dataset to find the counterfactual explanation for\n",
    "\n",
    "    threshold : int, optional\n",
    "        Max number of decision boundary points to generate. Default is 10000.\n",
    "\n",
    "    point_epsilon : float, optional\n",
    "        Precision used to estimate decision boundary points. Default is 0.1.\n",
    "\n",
    "    epsilon : float, optional\n",
    "        Step size used when displacing a point toward the decision boundary. Default is 0.01.\n",
    "\n",
    "    constraints : list, optional\n",
    "        A list of real-world constraints on the features (e.g., ranges, logic constraints). Default is [].\n",
    "\n",
    "    deltas : list, optional\n",
    "        Tolerances or maximum displacements for each continuous feature. Only works for continuous features. Default is [].\n",
    "\n",
    "    plot : boolean\n",
    "        Used as a parameter to determine whether to plot the results or not\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        A corrected point that satisfies the class change and real-world constraints.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    Exception\n",
    "        If the number of constraints exceeds the number of features.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This function trains the model on the provided dataset, generates boundary points using\n",
    "      `find_decision_boundary`, applies constraints, and finds the closest optimal point.\n",
    "    - Assumes binary classification and relies on external functions like `real_world_constraints`,\n",
    "      `closest_point`, `move_from_A_to_B_with_x1_displacement`, etc., which must be defined elsewhere.\n",
    "    - Includes plotting for visualization (e.g., boundary contours, points), which requires matplotlib.\n",
    "    - The function blends boundary approximation with counterfactual generation, useful for explainable AI.\n",
    "    - Print statements are for progress tracking; plotting is partially commented out but can be enabled.\n",
    "    - Usage: Call with a dataset and model to generate counterfactuals, e.g., for model interpretation or optimization.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> from sklearn.linear_model import LogisticRegression\n",
    "    >>> dataset = pd.DataFrame({'feat1': [0, 1, 2], 'feat2': [0, 1, 0], 'label': [0, 1, 0]})\n",
    "    >>> model = LogisticRegression()\n",
    "    >>> undesired_coords = [2, 0]  # Example point from class 0\n",
    "    >>> optimal = optimal_point(dataset, model, desired_class=1, original_class=0, undesired_coords=undesired_coords)\n",
    "    >>> print(optimal)  # e.g., array([[1.5, 0.5]])\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert categorical columns if needed (before balancing)\n",
    "    inv_col_map = convert_columns(dataset)\n",
    "\n",
    "    # Extract features and labels before balancing\n",
    "    X_orig = dataset.iloc[:, :-1]\n",
    "\n",
    "    # Save the original row's feature values\n",
    "    undesired_coords = X_orig.iloc[chosen_row, :].copy()\n",
    "\n",
    "    # Balance the dataset\n",
    "    dataset = balance_dataset(df=dataset, target=dataset.columns[-1])\n",
    "\n",
    "    if not check_class_balance(dataset, target=dataset.columns[-1]):\n",
    "        raise RuntimeError(\"Failed to balance classes for binary classification\")\n",
    "\n",
    "    sampled_dataset = dataset.sample(n=min(dataset.shape[0], 20000))\n",
    "\n",
    "    # Extract new training features/labels after balancing\n",
    "    X_train = sampled_dataset.iloc[:, :-1]\n",
    "    y_train = sampled_dataset.iloc[:, -1]\n",
    "    # Train the model\n",
    "    print(\"Fitting model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # STEP 2: Find decision boundary\n",
    "    # -------------------------------\n",
    "    print(\"boundary points started generation...\")\n",
    "\n",
    "    # This step uses binary interpolation to get points close to the decision boundary\n",
    "    boundary_points = multi_decision_boundary(model, X_train, y_train,\n",
    "                                             threshold=threshold, epsilon=point_epsilon)\n",
    "    print(\"boundary points finished.\")\n",
    "    print(boundary_points.shape)\n",
    "\n",
    "    # -------------------------------\n",
    "    # STEP 3: Apply real-world constraints (optional)\n",
    "    # -------------------------------\n",
    "    # Reduce boundary points based on external rules (e.g., cost limits, physics constraints)\n",
    "    contours_pd = real_world_constraints(points=boundary_points, undesired_coords=undesired_coords, constraints=constraints)\n",
    "\n",
    "    # contours = boundary_points  # (Commented: Alternative to use raw boundary)\n",
    "    undesired_datapt = np.reshape(np.array(list(undesired_coords)), (1, -1))  # Reshape undesired point to 2D array\n",
    "\n",
    "    # -------------------------------\n",
    "    # STEP 4: Find closest point on constrained boundary\n",
    "     # -------------------------------\n",
    "    if contours_pd is not None and desired_class != original_class:\n",
    "        contours = contours_pd.to_numpy()\n",
    "        print(\"Finding the closest point from the contour line to the point...\")\n",
    "        contours_pd.reset_index(drop=True, inplace=True)\n",
    "        optimal_datapt = closest_point(undesired_datapt, contour=contours)\n",
    "        print(\"Found the closest point from the contour line to the point.\")  # Note: Duplicate print, possibly a typo\n",
    "        D = optimal_datapt - undesired_datapt  # Compute direction vector\n",
    "        deltas = D * (1+epsilon)  # Scale by (1 + epsilon) to overshoot\n",
    "        optimal_datapt = move_from_A_to_B_with_x1_displacement(undesired_datapt, optimal_datapt, deltas=deltas)\n",
    "    elif desired_class == original_class or contours_pd is None:\n",
    "        # If we want to *stay within* the same class (more constrained)\n",
    "        all_constrained_feats = [var for (var,_) in constraints]\n",
    "        closest_boundedpt = None\n",
    "        vars = set(X_train.columns) - set(all_constrained_feats)\n",
    "        cont_mutable_vars = [X_train.columns.get_loc(col) for col in vars]\n",
    "        deltas, len_constr = det_constraints(datapt=undesired_datapt[0], vars=cont_mutable_vars, deltas=deltas)  # Determine constraints\n",
    "\n",
    "        if len_constr > X_train.shape[1]:\n",
    "            raise Exception(\"There cannot be more constraints than features\")\n",
    "        else:\n",
    "            # All n dimensions are constrained, so generate an exact grid of boundary candidates\n",
    "            bounded_contour_pts = get_multi_dim_border_points(center=undesired_datapt[0],\n",
    "                                                              extents=deltas,\n",
    "                                                              step=0.1)\n",
    "            np_bounded_contour = np.array(bounded_contour_pts)  # Convert to NumPy array\n",
    "            if plot:\n",
    "                x_values, y_values = np_bounded_contour[:, 0], np_bounded_contour[:, 1]  # Extract x/y for plotting\n",
    "                plt.scatter(x_values, y_values, marker='o')  # Plot bounded points\n",
    "\n",
    "            closest_boundedpt = closest_border_point(np_bounded_contour, contour=boundary_points)  # Find closest on border\n",
    "            print(closest_boundedpt)\n",
    "        D = closest_boundedpt - undesired_datapt  # Compute direction\n",
    "        optimal_datapt = move_from_A_to_B_with_x1_displacement(undesired_datapt, closest_boundedpt, deltas=D)  # Move point\n",
    "\n",
    "    # Plot original and optimal points with connecting line\n",
    "    if plot and contours_pd is not None:\n",
    "        contours = contours_pd.to_numpy()\n",
    "        params = {'mathtext.default': 'regular' }\n",
    "        plt.rcParams.update(params)\n",
    "        plt.scatter(contours[:,0], contours[:,1], lw=0.5, color='purple', label=\"Decision Boundary Points\")  # Commented: Plot contours for visualization\n",
    "        plt.scatter(undesired_datapt[0][0], undesired_datapt[0][1], c = 'r', label=\"NH: Not Healthy\")  # Plot undesired point\n",
    "        plt.text(undesired_datapt[0][0]+0.002, undesired_datapt[0][1]+0.002, 'NH')  # Label 'NH' (e.g., Non-Healthy)\n",
    "        plt.scatter(optimal_datapt[0][0], optimal_datapt[0][1], c = 'g', label=\"H: Healthy\")  # Plot optimal point (changed to green for distinction)\n",
    "        plt.text(optimal_datapt[0][0]+0.002, optimal_datapt[0][1]+0.002, 'NH')  # Label 'H' (e.g., Healthy; adjusted from duplicate 'NH')\n",
    "        plt.plot([undesired_datapt[0][0], optimal_datapt[0][0]], [undesired_datapt[0][1],optimal_datapt[0][1]], linestyle='--')  # Dashed line between points\n",
    "        red_patch = mpatches.Patch(color='red', label='Not Healthy')\n",
    "        blue_patch = mpatches.Patch(color='blue', label='Healthy')\n",
    "        green_patch = mpatches.Patch(color='green', label=\"Counterfactual\")\n",
    "        purple_patch = mpatches.Patch(color='purple', label='Decision Boundary Point')\n",
    "        plt.legend(loc='lower left', handles=[red_patch, blue_patch, purple_patch, green_patch])\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.title(\"Toy Dataset\")\n",
    "        plt.show()\n",
    "\n",
    "    categorical_features = [col for col in inv_col_map.keys()]\n",
    "    final_optimal_datapt = []\n",
    "\n",
    "    for col in X_train.columns:\n",
    "        if col in categorical_features:\n",
    "            idx = int(optimal_datapt[0,X_train.columns.get_loc(col)])\n",
    "            final_optimal_datapt.append(inv_col_map[col][idx])\n",
    "        else:\n",
    "            final_optimal_datapt.append(optimal_datapt[0,X_train.columns.get_loc(col)])\n",
    "\n",
    "    query_instance = undesired_coords\n",
    "    return dataset, model, query_instance, final_optimal_datapt, euclidean_distance(undesired_datapt, optimal_datapt), deltas[0], boundary_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "404129ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp_vec_per_axis(v, ref_point, bool_vec, frac=0.05):\n",
    "    \"\"\"\n",
    "    Clamp displacement vector v per-axis so endpoint p0+v stays inside +/- frac*|p0_i| (plus eps).\n",
    "\n",
    "    Args:\n",
    "      v    : array-like shape (n,)\n",
    "      ref_point   : array-like shape (n,)\n",
    "      bool_vec : boolean vector for isolating categorical features out of changes\n",
    "      frac : fraction (default 0.05)\n",
    "\n",
    "    Returns:\n",
    "      a clipped vector that is bounded within half of the interval on both sides for each dimension\n",
    "    \"\"\"\n",
    "    # allowable bounds for the final point\n",
    "    lower_bounds = (ref_point - (frac/2) * np.abs(ref_point)) * bool_vec\n",
    "    upper_bounds = (ref_point + (frac/2) * np.abs(ref_point)) * bool_vec\n",
    "\n",
    "    # clamp the endpoint\n",
    "    endpoint = np.clip(ref_point + v, lower_bounds, upper_bounds)\n",
    "    return np.clip(endpoint, lower_bounds, upper_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021abd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dice_cfs(df, model, query_instance, method, continuous_features, categorical_features, target, chosen_row, contours, plot=False, total_CFs=1, delta=100):\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    x_train = df.iloc[:, :-1]\n",
    "    backend='sklearn'\n",
    "\n",
    "    d = dice_ml.Data(dataframe=df, continuous_features=continuous_features, categorical_features=categorical_features, outcome_name=target)\n",
    "    m = dice_ml.Model(model=model, backend=backend)\n",
    "\n",
    "    exp_dice = dice_ml.Dice(d, m, method=method)\n",
    "\n",
    "    query_instance = x_train.iloc[[chosen_row]].to_numpy()\n",
    "\n",
    "    dice_cfs = exp_dice.generate_counterfactuals(pd.DataFrame(data=query_instance, columns=x_train.columns),\n",
    "                                                        total_CFs=total_CFs, desired_class=\"opposite\")\n",
    "\n",
    "    cfs_list = json.loads(dice_cfs.to_json())['cfs_list']\n",
    "    dist_cfs = []\n",
    "\n",
    "    # np_bounded_contour = np.array(bounded_contour_pts)  # Convert to NumPy array\n",
    "    # x_values, y_values = np_bounded_contour[:, 0], np_bounded_contour[:, 1]  # Extract x/y for plotting\n",
    "    # if plot:\n",
    "    #     plt.scatter(x_values, y_values, marker='o')  # Plot bounded points\n",
    "    contours = contours.reset_index(drop=True)\n",
    "\n",
    "    bool_vec = []\n",
    "    for col in df.iloc[:, :-1].columns:\n",
    "        if np.issubdtype(df[col].dtype, np.number):\n",
    "            bool_vec.append(1)   # numeric -> allow changes\n",
    "        else:\n",
    "            bool_vec.append(0)   # categorical -> mask out\n",
    "\n",
    "    if delta.all() == 100:\n",
    "        for point in cfs_list[0]:\n",
    "            point_vec = [float(point[i]) for i in range(len(point[:-1]))]\n",
    "            dist_cfs.append(euclidean_distance(np.array(point_vec), query_instance))\n",
    "    else:\n",
    "        for point in cfs_list[0]:\n",
    "            point_vec = [float(point[i]) for i in range(len(point[:-1]))]\n",
    "            point_vec = np.reshape(np.array(point_vec), (1, -1))\n",
    "            endpoint_vec = clamp_vec_per_axis(point_vec, ref_point=query_instance, bool_vec=bool_vec, frac=delta/100)\n",
    "            closest_pt = closest_point(endpoint_vec, contour=contours.to_numpy())\n",
    "            dist_cfs.append(euclidean_distance(closest_pt, endpoint_vec))\n",
    "\n",
    "    # if plot:\n",
    "    #     for point in cfs_list[0][:5]:\n",
    "    #         x,y = point[0], point[1]\n",
    "    #         print(\"EUCLIDEAN DISTANCE:\", euclidean_distance(delta*np.array((x,y)), query_instance))\n",
    "    #         plt.scatter(x,y, c = 'yellow')  # Plot optimal point (changed to green for distinction)\n",
    "    #         plt.text(x+0.002, y+0.002, 'H')  # Label 'H' (e.g., Healthy; adjusted from duplicate 'NH')\n",
    "    #         plt.plot([x,query_instance[0][0]], [y, query_instance[0][1]], linestyle='--')  # Dashed line between points\n",
    "    end = datetime.datetime.now()\n",
    "    diff = end - start\n",
    "    print(f\"Elapsed time: {diff}\")\n",
    "\n",
    "    return dist_cfs, diff.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d0ba61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exps(dataset, model, method, target, x_train, y_train, continuous_features, categorical_features, inv_map, num_samples, delta=100, constraints=[], threshold=25000):\n",
    "    dice_dists, optimal_dists = [], []\n",
    "    sub_dataset = dataset[dataset[target] == 1]\n",
    "    random_integers = random.sample(range(0, sub_dataset.shape[0]-1), num_samples)\n",
    "\n",
    "    for i in random_integers:\n",
    "        real_idx = sub_dataset.index[i]\n",
    "        chosen_row=real_idx\n",
    "        query_instance=x_train.iloc[chosen_row:chosen_row+1,:]\n",
    "        label = y_train.iloc[chosen_row:chosen_row+1]\n",
    "        df, model, query_instance, opt_point, dist, exp_delta, boundary_points = optimal_point(dataset, model, desired_class=0, original_class=1, threshold=threshold, chosen_row=chosen_row, point_epsilon=1e-3, epsilon=0.01, constraints=constraints, deltas=[delta] * x_train.shape[1])\n",
    "        optimal_dists.append(dist)\n",
    "        dist_cfs, _ = run_dice_cfs(df=df, model=model, query_instance=query_instance,method=method, continuous_features=continuous_features, categorical_features=categorical_features, target=target, contours=boundary_points, chosen_row=chosen_row, delta=exp_delta, total_CFs=10)\n",
    "        dice_dists.extend(dist_cfs)\n",
    "    return optimal_dists, dice_dists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d4aab4",
   "metadata": {},
   "source": [
    "# Step 3: Toy Dataset\n",
    "\n",
    "We run a few experiments using the toy dataset, and we compare the results visually using both the optimal point method and the dice model-agnostic methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "400db22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_map = {\n",
    "    1: -1,\n",
    "    -1: 1\n",
    "}\n",
    "x_train = df.iloc[:,:-1]\n",
    "y_train  = df.iloc[:,-1]\n",
    "continuous_features=['x1', 'x2']\n",
    "categorical_features=[]\n",
    "target='y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52bca57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Compuworld/drexel_research_2024_2025/Docs/files/common_functions.py:426: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  balanced_dataset = pd.concat([balanced_dataset, upsampled_class], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts:\n",
      " y\n",
      "-1    10\n",
      " 1    10\n",
      "Name: count, dtype: int64\n",
      "Fitting model...\n",
      "Model training complete.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(100, 2)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Found the closest point from the contour line to the point.\n",
      "[[1.00640294 1.00938876]]\n",
      "[[0.27978516 1.65136719]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 44.74it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m optimal_dists, dice_dists  = \u001b[43mexps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43msvm_classifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mkdtree\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mcontinuous_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontinuous_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mcategorical_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategorical_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43minv_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43minv_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mexps\u001b[39m\u001b[34m(dataset, model, method, target, x_train, y_train, continuous_features, categorical_features, inv_map, num_samples, delta, constraints, threshold)\u001b[39m\n\u001b[32m     11\u001b[39m     df, model, query_instance, opt_point, dist, exp_delta, boundary_points = optimal_point(dataset, model, desired_class=\u001b[32m0\u001b[39m, original_class=\u001b[32m1\u001b[39m, threshold=threshold, chosen_row=chosen_row, point_epsilon=\u001b[32m1e-3\u001b[39m, epsilon=\u001b[32m0.01\u001b[39m, constraints=constraints, deltas=[delta] * x_train.shape[\u001b[32m1\u001b[39m])\n\u001b[32m     12\u001b[39m     optimal_dists.append(dist)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     dist_cfs, _ = \u001b[43mrun_dice_cfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontinuous_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontinuous_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategorical_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontours\u001b[49m\u001b[43m=\u001b[49m\u001b[43mboundary_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchosen_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchosen_row\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexp_delta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_CFs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     dice_dists.extend(dist_cfs)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m optimal_dists, dice_dists\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mrun_dice_cfs\u001b[39m\u001b[34m(df, model, query_instance, method, continuous_features, categorical_features, target, chosen_row, contours, plot, total_CFs, delta)\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     31\u001b[39m         bool_vec.append(\u001b[32m0\u001b[39m)   \u001b[38;5;66;03m# categorical -> mask out\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m delta == \u001b[32m100\u001b[39m:\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m point \u001b[38;5;129;01min\u001b[39;00m cfs_list[\u001b[32m0\u001b[39m]:\n\u001b[32m     35\u001b[39m         point_vec = [\u001b[38;5;28mfloat\u001b[39m(point[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(point[:-\u001b[32m1\u001b[39m]))]\n",
      "\u001b[31mValueError\u001b[39m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "optimal_dists, dice_dists  = exps(df, model=svm_classifier, method='kdtree',\n",
    "                                 x_train=x_train, y_train=y_train,\n",
    "                                 continuous_features=continuous_features,\n",
    "                                 categorical_features=categorical_features,\n",
    "                                 inv_map=inv_map, num_samples=9,\n",
    "                                 target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f458e94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0743510584156337 8.871731684849527\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e136c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Compuworld/drexel_research_2024_2025/Docs/files/common_functions.py:426: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  balanced_dataset = pd.concat([balanced_dataset, upsampled_class], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts:\n",
      " y\n",
      "-1    10\n",
      " 1    10\n",
      "Name: count, dtype: int64\n",
      "Fitting model...\n",
      "Model training complete.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(100, 2)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Found the closest point from the contour line to the point.\n",
      "[[1.00500425 1.0095964 ]]\n",
      "[[0.20117188 2.50146484]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.45it/s]\n",
      "/mnt/c/Users/Compuworld/drexel_research_2024_2025/Docs/files/common_functions.py:426: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  balanced_dataset = pd.concat([balanced_dataset, upsampled_class], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:00.045477\n",
      "Class counts:\n",
      " y\n",
      "-1    10\n",
      " 1    10\n",
      "Name: count, dtype: int64\n",
      "Fitting model...\n",
      "Model training complete.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(100, 2)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Found the closest point from the contour line to the point.\n",
      "[[1.00633279 1.0089495 ]]\n",
      "[[0.27441406 0.96044922]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.26it/s]\n",
      "/mnt/c/Users/Compuworld/drexel_research_2024_2025/Docs/files/common_functions.py:426: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  balanced_dataset = pd.concat([balanced_dataset, upsampled_class], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:00.046169\n",
      "Class counts:\n",
      " y\n",
      "-1    10\n",
      " 1    10\n",
      "Name: count, dtype: int64\n",
      "Fitting model...\n",
      "Model training complete.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(100, 2)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Found the closest point from the contour line to the point.\n",
      "[[1.00640294 1.00938876]]\n",
      "[[0.27978516 1.65136719]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.34it/s]\n",
      "/mnt/c/Users/Compuworld/drexel_research_2024_2025/Docs/files/common_functions.py:426: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  balanced_dataset = pd.concat([balanced_dataset, upsampled_class], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:00.049023\n",
      "Class counts:\n",
      " y\n",
      "-1    10\n",
      " 1    10\n",
      "Name: count, dtype: int64\n",
      "Fitting model...\n",
      "Model training complete.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(100, 2)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Found the closest point from the contour line to the point.\n",
      "[[1.00861413 1.00746214]]\n",
      "[[0.7277832  0.39697266]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.91it/s]\n",
      "/mnt/c/Users/Compuworld/drexel_research_2024_2025/Docs/files/common_functions.py:426: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  balanced_dataset = pd.concat([balanced_dataset, upsampled_class], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:00.042534\n",
      "Class counts:\n",
      " y\n",
      "-1    10\n",
      " 1    10\n",
      "Name: count, dtype: int64\n",
      "Fitting model...\n",
      "Model training complete.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(100, 2)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Found the closest point from the contour line to the point.\n",
      "[[1.00841258 1.00920567]]\n",
      "[[0.63525391 1.27050781]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 27.62it/s]\n",
      "/mnt/c/Users/Compuworld/drexel_research_2024_2025/Docs/files/common_functions.py:426: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  balanced_dataset = pd.concat([balanced_dataset, upsampled_class], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:00.039564\n",
      "Class counts:\n",
      " y\n",
      "-1    10\n",
      " 1    10\n",
      "Name: count, dtype: int64\n",
      "Fitting model...\n",
      "Model training complete.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(100, 2)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Found the closest point from the contour line to the point.\n",
      "[[1.0054242  1.00932204]]\n",
      "[[0.21972656 1.48876953]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 29.24it/s]\n",
      "/mnt/c/Users/Compuworld/drexel_research_2024_2025/Docs/files/common_functions.py:426: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  balanced_dataset = pd.concat([balanced_dataset, upsampled_class], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:00.037550\n",
      "Class counts:\n",
      " y\n",
      "-1    10\n",
      " 1    10\n",
      "Name: count, dtype: int64\n",
      "Fitting model...\n",
      "Model training complete.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(100, 2)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Found the closest point from the contour line to the point.\n",
      "[[1.00833863 1.00092963]]\n",
      "[[0.60693359 0.11035156]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.37it/s]\n",
      "/mnt/c/Users/Compuworld/drexel_research_2024_2025/Docs/files/common_functions.py:426: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  balanced_dataset = pd.concat([balanced_dataset, upsampled_class], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:00.045163\n",
      "Class counts:\n",
      " y\n",
      "-1    10\n",
      " 1    10\n",
      "Name: count, dtype: int64\n",
      "Fitting model...\n",
      "Model training complete.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(100, 2)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Found the closest point from the contour line to the point.\n",
      "[[1.01671689 1.00970858]]\n",
      "[[-0.15136719  3.46484375]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 30.04it/s]\n",
      "/mnt/c/Users/Compuworld/drexel_research_2024_2025/Docs/files/common_functions.py:426: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  balanced_dataset = pd.concat([balanced_dataset, upsampled_class], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:00.036752\n",
      "Class counts:\n",
      " y\n",
      "-1    10\n",
      " 1    10\n",
      "Name: count, dtype: int64\n",
      "Fitting model...\n",
      "Model training complete.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(100, 2)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Found the closest point from the contour line to the point.\n",
      "[[0.         1.00981485]]\n",
      "[[0.         5.45410156]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 28.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:00.038440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimal_dists, dice_dists  = exps(df, model=svm_classifier, method='random',\n",
    "                                 x_train=x_train, y_train=y_train,\n",
    "                                 continuous_features=continuous_features,\n",
    "                                 categorical_features=categorical_features,\n",
    "                                 inv_map=inv_map, num_samples=9,\n",
    "                                 target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59d4e603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0723440122846277 8.466144414889746\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82fed399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Compuworld/drexel_research_2024_2025/Docs/files/common_functions.py:426: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  balanced_dataset = pd.concat([balanced_dataset, upsampled_class], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts:\n",
      " y\n",
      "-1    10\n",
      " 1    10\n",
      "Name: count, dtype: int64\n",
      "Fitting model...\n",
      "Model training complete.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(100, 2)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Found the closest point from the contour line to the point.\n",
      "[[1.00640294 1.00938876]]\n",
      "[[0.27978516 1.65136719]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "/mnt/c/Users/Compuworld/drexel_research_2024_2025/Docs/files/common_functions.py:426: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  balanced_dataset = pd.concat([balanced_dataset, upsampled_class], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:00.084651\n",
      "Class counts:\n",
      " y\n",
      "-1    10\n",
      " 1    10\n",
      "Name: count, dtype: int64\n",
      "Fitting model...\n",
      "Model training complete.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(100, 2)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Found the closest point from the contour line to the point.\n",
      "[[1.00815733 1.0083413 ]]\n",
      "[[0.54711914 0.60791016]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.49it/s]\n",
      "/mnt/c/Users/Compuworld/drexel_research_2024_2025/Docs/files/common_functions.py:426: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  balanced_dataset = pd.concat([balanced_dataset, upsampled_class], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:00.083807\n",
      "Class counts:\n",
      " y\n",
      "-1    10\n",
      " 1    10\n",
      "Name: count, dtype: int64\n",
      "Fitting model...\n",
      "Model training complete.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(100, 2)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Found the closest point from the contour line to the point.\n",
      "[[1.00500425 1.0095964 ]]\n",
      "[[0.20117188 2.50146484]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.24it/s]\n",
      "/mnt/c/Users/Compuworld/drexel_research_2024_2025/Docs/files/common_functions.py:426: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  balanced_dataset = pd.concat([balanced_dataset, upsampled_class], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:00.085289\n",
      "Class counts:\n",
      " y\n",
      "-1    10\n",
      " 1    10\n",
      "Name: count, dtype: int64\n",
      "Fitting model...\n",
      "Model training complete.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(100, 2)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Found the closest point from the contour line to the point.\n",
      "[[1.00825304 1.0047772 ]]\n",
      "[[0.57714844 0.19238281]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.66it/s]\n",
      "/mnt/c/Users/Compuworld/drexel_research_2024_2025/Docs/files/common_functions.py:426: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  balanced_dataset = pd.concat([balanced_dataset, upsampled_class], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:00.605787\n",
      "Class counts:\n",
      " y\n",
      "-1    10\n",
      " 1    10\n",
      "Name: count, dtype: int64\n",
      "Fitting model...\n",
      "Model training complete.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(100, 2)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Found the closest point from the contour line to the point.\n",
      "[[1.00888434 1.00910728]]\n",
      "[[0.90429688 1.13037109]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.73it/s]\n",
      "/mnt/c/Users/Compuworld/drexel_research_2024_2025/Docs/files/common_functions.py:426: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  balanced_dataset = pd.concat([balanced_dataset, upsampled_class], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:00.076006\n",
      "Class counts:\n",
      " y\n",
      "-1    10\n",
      " 1    10\n",
      "Name: count, dtype: int64\n",
      "Fitting model...\n",
      "Model training complete.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(100, 2)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Found the closest point from the contour line to the point.\n",
      "[[1.00633279 1.0089495 ]]\n",
      "[[0.27441406 0.96044922]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.49it/s]\n",
      "/mnt/c/Users/Compuworld/drexel_research_2024_2025/Docs/files/common_functions.py:426: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  balanced_dataset = pd.concat([balanced_dataset, upsampled_class], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:00.077613\n",
      "Class counts:\n",
      " y\n",
      "-1    10\n",
      " 1    10\n",
      "Name: count, dtype: int64\n",
      "Fitting model...\n",
      "Model training complete.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(100, 2)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Found the closest point from the contour line to the point.\n",
      "[[1.01636549 1.00931052]]\n",
      "[[-0.15966797  1.46386719]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "/mnt/c/Users/Compuworld/drexel_research_2024_2025/Docs/files/common_functions.py:426: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  balanced_dataset = pd.concat([balanced_dataset, upsampled_class], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:00.083416\n",
      "Class counts:\n",
      " y\n",
      "-1    10\n",
      " 1    10\n",
      "Name: count, dtype: int64\n",
      "Fitting model...\n",
      "Model training complete.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(100, 2)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Found the closest point from the contour line to the point.\n",
      "[[1.00269212 1.00981475]]\n",
      "[[0.13720703 5.45117188]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.59it/s]\n",
      "/mnt/c/Users/Compuworld/drexel_research_2024_2025/Docs/files/common_functions.py:426: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  balanced_dataset = pd.concat([balanced_dataset, upsampled_class], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:00.082902\n",
      "Class counts:\n",
      " y\n",
      "-1    10\n",
      " 1    10\n",
      "Name: count, dtype: int64\n",
      "Fitting model...\n",
      "Model training complete.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(100, 2)\n",
      "Finding the closest point from the contour line to the point...\n",
      "Found the closest point from the contour line to the point.\n",
      "[[1.01636549 1.0097085 ]]\n",
      "[[-0.15966797  3.46386719]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:00.081730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimal_dists, dice_dists  = exps(df, model=svm_classifier, method='genetic',\n",
    "                                 x_train=x_train, y_train=y_train,\n",
    "                                 continuous_features=continuous_features,\n",
    "                                 categorical_features=categorical_features,\n",
    "                                 inv_map=inv_map, num_samples=9,\n",
    "                                 target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfbf6ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0693472948856804 5.588572958721644\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dda666",
   "metadata": {},
   "source": [
    "# Step 3: Adult Income Dataset Experiments\n",
    "\n",
    "We run a few experiments using the adult income dataset comparing DiCE model-agnostic methodologies and the Optimal Point method.\n",
    "\n",
    "We follow the following steps: \n",
    "\n",
    "1. Import the dataset using the helpers function from DiCE \n",
    "2. Initialize the classifier which is a Random Forest Classifier in this case\n",
    "3. Iterate for 50 or 100 randomly selected points using the Optimal point method \n",
    "4. After each iteration of generation with the optimal point method, we apply the run_dice_cfs method that enables us to generate counterfactuals using DiCE's specific model-agnostic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da3c1155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danielsin/miniconda3/envs/gpu_env/lib/python3.12/site-packages/dice_ml/utils/helpers.py:79: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  adult_data = adult_data.replace({'income': {'<=50K': 0, '>50K': 1}})\n"
     ]
    }
   ],
   "source": [
    "dataset = helpers.load_adult_income_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c5b3ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Single</td>\n",
       "      <td>White-Collar</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>Self-Employed</td>\n",
       "      <td>Assoc</td>\n",
       "      <td>Married</td>\n",
       "      <td>Professional</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Married</td>\n",
       "      <td>White-Collar</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Single</td>\n",
       "      <td>Service</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Self-Employed</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Married</td>\n",
       "      <td>White-Collar</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age      workclass     education marital_status    occupation   race  \\\n",
       "0   28        Private     Bachelors         Single  White-Collar  White   \n",
       "1   30  Self-Employed         Assoc        Married  Professional  White   \n",
       "2   32        Private  Some-college        Married  White-Collar  White   \n",
       "3   20        Private  Some-college         Single       Service  White   \n",
       "4   41  Self-Employed  Some-college        Married  White-Collar  White   \n",
       "\n",
       "   gender  hours_per_week  income  \n",
       "0  Female              60       0  \n",
       "1    Male              65       1  \n",
       "2    Male              50       0  \n",
       "3  Female              35       0  \n",
       "4    Male              50       0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d801216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5c921bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = dataset.iloc[:,:-1]\n",
    "y_train = dataset.iloc[:,-1]\n",
    "continuous_features=[\"age\", \"hours_per_week\"]\n",
    "categorical_features = ['marital_status', 'workclass', 'education', 'race', 'gender', 'occupation']\n",
    "target='income'\n",
    "inv_map = {\n",
    "    0: 1,\n",
    "    1: 0\n",
    "}\n",
    "constraints = [\n",
    "    (\"age\", \"equal\"),\n",
    "    (\"workclass\", \"equal\"),\n",
    "    (\"education\", \"equal\"),\n",
    "    (\"race\", \"equal\"),\n",
    "    (\"gender\", \"equal\"),\n",
    "    (\"occupation\", \"equal\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fdf419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Compuworld/drexel_research_2024_2025/Docs/files/common_functions.py:426: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  balanced_dataset = pd.concat([balanced_dataset, upsampled_class], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts:\n",
      " income\n",
      "0    19820\n",
      "1    19820\n",
      "Name: count, dtype: int64\n",
      "Fitting model...\n",
      "Model training complete.\n",
      "boundary points started generation...\n",
      "boundary points finished.\n",
      "(25000, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Compuworld/drexel_research_2024_2025/Docs/files/common_functions.py:379: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  select_pts = points.loc[points[constraint[0]] == undesired_coords[points.columns.get_loc(constraint[0])], :]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'deltas' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m optimal_dists, dice_dists = \u001b[43mexps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mkdtree\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mcontinuous_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontinuous_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43mcategorical_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategorical_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43minv_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43minv_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstraints\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mexps\u001b[39m\u001b[34m(dataset, model, method, target, x_train, y_train, continuous_features, categorical_features, inv_map, num_samples, delta, constraints, threshold)\u001b[39m\n\u001b[32m      9\u001b[39m query_instance=x_train.iloc[chosen_row:chosen_row+\u001b[32m1\u001b[39m,:]\n\u001b[32m     10\u001b[39m label = y_train.iloc[chosen_row:chosen_row+\u001b[32m1\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m df, model, query_instance, opt_point, dist, exp_delta, boundary_points = \u001b[43moptimal_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesired_class\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_class\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchosen_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchosen_row\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoint_epsilon\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m optimal_dists.append(dist)\n\u001b[32m     13\u001b[39m dist_cfs, _ = run_dice_cfs(df=df, model=model, query_instance=query_instance,method=method, continuous_features=continuous_features, categorical_features=categorical_features, target=target, contours=boundary_points, chosen_row=chosen_row, delta=exp_delta, total_CFs=\u001b[32m10\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 149\u001b[39m, in \u001b[36moptimal_point\u001b[39m\u001b[34m(dataset, model, desired_class, original_class, chosen_row, threshold, point_epsilon, epsilon, constraints, delta, plot, step)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28mvars\u001b[39m = \u001b[38;5;28mset\u001b[39m(X_train.columns) - \u001b[38;5;28mset\u001b[39m(all_constrained_feats)\n\u001b[32m    148\u001b[39m cont_mutable_vars = [X_train.columns.get_loc(col) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m deltas, len_constr = det_constraints(datapt=undesired_datapt[\u001b[32m0\u001b[39m], \u001b[38;5;28mvars\u001b[39m=cont_mutable_vars, deltas=\u001b[43mdeltas\u001b[49m)  \u001b[38;5;66;03m# Determine constraints\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m len_constr > X_train.shape[\u001b[32m1\u001b[39m]:\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThere cannot be more constraints than features\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'deltas' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "optimal_dists, dice_dists = exps(dataset, clf, 'kdtree', target=target,\n",
    "                                  x_train=x_train, y_train=y_train,\n",
    "                                  continuous_features=continuous_features,\n",
    "                                    categorical_features=categorical_features,\n",
    "                                    inv_map=inv_map, num_samples=500, delta=20,\n",
    "                                    constraints=constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4996a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e73a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dists, dice_dists = exps(dataset, clf, 'random', target=target,\n",
    "                                  x_train=x_train, y_train=y_train,\n",
    "                                  continuous_features=continuous_features,\n",
    "                                    categorical_features=categorical_features,\n",
    "                                    inv_map=inv_map, num_samples=500, delta=20,\n",
    "                                    constraints=constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fe2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad741871",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dists, dice_dists = exps(dataset, clf, 'genetic', target=target,\n",
    "                                  x_train=x_train, y_train=y_train,\n",
    "                                  continuous_features=continuous_features,\n",
    "                                    categorical_features=categorical_features,\n",
    "                                    inv_map=inv_map, num_samples=500, delta=20,\n",
    "                                    constraints=constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cde62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39df6465",
   "metadata": {},
   "source": [
    "# Step 3: Heart Disease Dataset Experiments\n",
    "\n",
    "We run a few experiments using the heart disease dataset comparing DiCE model-agnostic methodologies and the Optimal Point method.\n",
    "\n",
    "We follow the following steps: \n",
    "\n",
    "1. Import the dataset using the helpers function from DiCE \n",
    "2. Initialize the classifier which is a Random Forest Classifier in this case\n",
    "3. Iterate for 50 or 100 randomly selected points using the Optimal point method \n",
    "4. After each iteration of generation with the optimal point method, we apply the run_dice_cfs method that enables us to generate counterfactuals using DiCE's specific model-agnostic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb7e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease = pd.read_csv(\n",
    "'../../heart.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96881f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(heart_disease.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26c0db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = [\n",
    "    (\"age\", \"equal\"),\n",
    "    (\"sex\", \"equal\"),\n",
    "    (\"cp\", \"equal\"),\n",
    "    (\"fbs\", \"equal\"),\n",
    "    (\"restecg\", \"equal\"),\n",
    "    (\"exang\", \"equal\"),\n",
    "    (\"slope\", \"equal\"),\n",
    "    (\"thal\", \"equal\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b34b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = heart_disease.iloc[:,:-1]\n",
    "y_train = heart_disease.iloc[:,-1]\n",
    "continuous_features=[\"age\", \"trestbps\", \"thalach\", \"oldpeak\", \"chol\"]\n",
    "categorical_features=['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n",
    "target='target'\n",
    "inv_map = {\n",
    "    0: 1,\n",
    "    1: 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671bd6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dists, dice_dists = exps(dataset=heart_disease, model=svm_classifier, method='kdtree', target=target,\n",
    "                                  x_train=x_train, y_train=y_train,\n",
    "                                  continuous_features=continuous_features,\n",
    "                                    categorical_features=categorical_features,\n",
    "                                    inv_map=inv_map, num_samples=100, delta=20,\n",
    "                                    constraints=constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a93d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dists, dice_dists = exps(dataset=heart_disease, model=svm_classifier, method='random', target=target,\n",
    "                                  x_train=x_train, y_train=y_train,\n",
    "                                  continuous_features=continuous_features,\n",
    "                                    categorical_features=categorical_features,\n",
    "                                    inv_map=inv_map, num_samples=500, delta=20,\n",
    "                                    constraints=constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a163fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f8329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dists, dice_dists = exps(dataset=heart_disease, model=svm_classifier, method='genetic', target=target,\n",
    "                                  x_train=x_train, y_train=y_train,\n",
    "                                  continuous_features=continuous_features,\n",
    "                                    categorical_features=categorical_features,\n",
    "                                    inv_map=inv_map, num_samples=500, delta=20,\n",
    "                                    constraints=constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2486f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e3f48b",
   "metadata": {},
   "source": [
    "# Comparing DiCE with large feature dataset made of numerical features \n",
    "\n",
    "We use the ```make_classification``` function of sci-kit learn library to generate a synthetic dataset of numerical values that we can then compare both methodologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3bea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=2000, n_features=20, n_informative=20, n_redundant=0, random_state=42, n_classes=2)\n",
    "y = y.reshape(-1,1)\n",
    "columns = [\"x\"+str(i) for i in range(20)]\n",
    "columns.append('y')\n",
    "dataset = pd.DataFrame(data=np.hstack((X,y)), columns=columns)\n",
    "model = LogisticRegression()\n",
    "continuous_features = columns[:-1]\n",
    "categorical_features=[]\n",
    "target = 'y'\n",
    "x_train = dataset.iloc[:,:-1]\n",
    "y_train = dataset.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dists, dice_dists = exps(dataset=dataset, model=model,\n",
    "                                 method='kdtree', target=target,\n",
    "                                  x_train=x_train, y_train=y_train,\n",
    "                                  continuous_features=continuous_features,\n",
    "                                    categorical_features=categorical_features,\n",
    "                                    inv_map=inv_map, num_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efb540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a644f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dists, dice_dists = exps(dataset=dataset, model=model,\n",
    "                                 method='random', target=target,\n",
    "                                  x_train=x_train, y_train=y_train,\n",
    "                                  continuous_features=continuous_features,\n",
    "                                    categorical_features=categorical_features,\n",
    "                                    inv_map=inv_map, num_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51f0c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b1820",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dists, dice_dists = exps(dataset=dataset, model=model,\n",
    "                                 method='genetic', target=target,\n",
    "                                  x_train=x_train, y_train=y_train,\n",
    "                                  continuous_features=continuous_features,\n",
    "                                    categorical_features=categorical_features,\n",
    "                                    inv_map=inv_map, num_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af3fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(optimal_dists), np.mean(dice_dists))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6830972b",
   "metadata": {},
   "source": [
    "# Step 4: Runtime Tests \n",
    "\n",
    "The function ```runtime_tests()``` are used for comparing DiCE's model-agnostic approaches and Optimal Point for time complexity. We use a logistic regression classifier for examining runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c997f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runtime_tests(number_of_features, method, total_random=100):\n",
    "    X, y = make_classification(n_samples=5000, n_features=number_of_features, n_informative=number_of_features,\n",
    "                            n_redundant=0, n_classes=2, random_state=42)\n",
    "    y = np.reshape(y, (-1, 1))\n",
    "    columns = [\"x\"+str(i) for i in range(1, X.shape[1]+1)]\n",
    "    columns.append('y')\n",
    "    dataset = pd.DataFrame(data=np.hstack((X,y)), columns=columns)\n",
    "    continuous_features=[\"x\"+str(i) for i in range(1, X.shape[1]+1)]\n",
    "    target='y'\n",
    "    inv_map = {\n",
    "        0: 1,\n",
    "        1: 0\n",
    "    }\n",
    "    dice_dists, optimal_dists = [], []\n",
    "    dice_runtime = []\n",
    "    sub_dataset = dataset[dataset[target] == 0]\n",
    "    random_integers = random.sample(range(1, sub_dataset.shape[0]), total_random)\n",
    "    clf = LogisticRegression()\n",
    "\n",
    "    for i in random_integers:\n",
    "        real_idx = sub_dataset.index[i]\n",
    "        chosen_row=real_idx\n",
    "        query_instance=X[chosen_row:chosen_row+1,:]\n",
    "        label = y[chosen_row:chosen_row+1]\n",
    "        df, model, query_instance, opt_point, dist,_, contours = optimal_point(dataset, clf, desired_class=inv_map[label.item()], original_class=label.item(), threshold=5000, chosen_row=chosen_row, point_epsilon=1e-3, epsilon=0.01, constraints=[])\n",
    "        optimal_dists.append(dist)\n",
    "        dist_cfs, total_seconds = run_dice_cfs(df=df, contours=contours, model=model,query_instance=query_instance,method=method, continuous_features=continuous_features, categorical_features=[], target=target, chosen_row=chosen_row)\n",
    "        dice_dists.extend(dist_cfs)\n",
    "        dice_runtime.append(total_seconds)\n",
    "\n",
    "    print(np.mean(dice_runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9c5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_tests(number_of_features=10, method='kdtree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c41bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_tests(number_of_features=50, method='kdtree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f5df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_tests(number_of_features=10, method='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_tests(number_of_features=50, method='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_tests(number_of_features=10, method='genetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a385e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_tests(number_of_features=50, method='genetic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
